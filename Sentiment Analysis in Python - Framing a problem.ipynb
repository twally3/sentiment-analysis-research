{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis in Python - Framing a problem\n",
    "By Max Taylor\n",
    "\n",
    "This is an overview of the full process of developing a Neural Network for sentiment analysis. We will address: How to develop and impement a predictive theory, prepare data, build the network, reduce noise and optimise. This will involve how to attack and solve the problem; Which can be applied throughout future networks.\n",
    "\n",
    "### Contents\n",
    "- Loading the dataset\n",
    "- predictive theory\n",
    "- Theory Validation\n",
    "- Preparing input and output data\n",
    "- Building the network\n",
    "- Identifying Neural Noise\n",
    "- Reducing Neural Noise\n",
    "- Analysing inefficiencies in the network\n",
    "- Optimising inefficiencies in the network\n",
    "- Further noise reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g = open('reviews.txt','r')\n",
    "reviews = list(map(lambda x:x[:-1],g.readlines()))\n",
    "g.close()\n",
    "\n",
    "g = open('labels.txt','r')\n",
    "labels = list(map(lambda x:x[:-1].upper(),g.readlines()))\n",
    "g.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life  such as  teachers  . my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers  . the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students . when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled . . . . . . . . . at . . . . . . . . . . high . a classic line inspector i  m here to sack one of your teachers . student welcome to bromwell high . i expect that many adults of my age think that bromwell high is far fetched . what a pity that it isn  t   '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'POSITIVE'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Develop a predictive theory\n",
    "\n",
    "Look over the data and consider the best way to use the input information to effectively get the output information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pretty_print_review_and_label(i):\n",
    "    print(labels[i] + \"\\t:\\t\" + reviews[i][:80] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels.txt \t : \t reviews.txt\n",
      "\n",
      "NEGATIVE\t:\tthis movie is terrible but it has some good effects .  ...\n",
      "POSITIVE\t:\tadrian pasdar is excellent is this film . he makes a fascinating woman .  ...\n",
      "NEGATIVE\t:\tcomment this movie is impossible . is terrible  very improbable  bad interpretat...\n",
      "POSITIVE\t:\texcellent episode movie ala pulp fiction .  days   suicides . it doesnt get more...\n",
      "NEGATIVE\t:\tif you haven  t seen this  it  s terrible . it is pure trash . i saw this about ...\n",
      "POSITIVE\t:\tthis schiffer guy is a real genius  the movie is of excellent quality and both e...\n"
     ]
    }
   ],
   "source": [
    "print(\"labels.txt \\t : \\t reviews.txt\\n\")\n",
    "pretty_print_review_and_label(2137)\n",
    "pretty_print_review_and_label(12816)\n",
    "pretty_print_review_and_label(6267)\n",
    "pretty_print_review_and_label(21934)\n",
    "pretty_print_review_and_label(5297)\n",
    "pretty_print_review_and_label(4998)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the data should be split into words as they convey the most meaning. For example, single characters do not contain any form of context and whole sentences are too large and general for the network to compute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory validation\n",
    "\n",
    "Having produced a predictive theory it can be helpful to validate the theory. In this case it was decided that words were the most helpful to us, so this is what we will test!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "positive_counts = Counter()\n",
    "negative_counts = Counter()\n",
    "total_counts = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(reviews)):\n",
    "    if labels[i] == 'POSITIVE':\n",
    "        for word in reviews[i].split(' '):\n",
    "            positive_counts[word] += 1\n",
    "            total_counts[word] += 1\n",
    "    else:\n",
    "        for word in reviews[i].split(' '):\n",
    "            negative_counts[word] += 1\n",
    "            total_counts[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 550468),\n",
       " ('the', 173324),\n",
       " ('.', 159654),\n",
       " ('and', 89722),\n",
       " ('a', 83688),\n",
       " ('of', 76855),\n",
       " ('to', 66746),\n",
       " ('is', 57245),\n",
       " ('in', 50215),\n",
       " ('br', 49235),\n",
       " ('it', 48025),\n",
       " ('i', 40743),\n",
       " ('that', 35630),\n",
       " ('this', 35080),\n",
       " ('s', 33815),\n",
       " ('as', 26308),\n",
       " ('with', 23247),\n",
       " ('for', 22416),\n",
       " ('was', 21917),\n",
       " ('film', 20937),\n",
       " ('but', 20822),\n",
       " ('movie', 19074),\n",
       " ('his', 17227),\n",
       " ('on', 17008),\n",
       " ('you', 16681),\n",
       " ('he', 16282),\n",
       " ('are', 14807),\n",
       " ('not', 14272),\n",
       " ('t', 13720),\n",
       " ('one', 13655),\n",
       " ('have', 12587),\n",
       " ('be', 12416),\n",
       " ('by', 11997),\n",
       " ('all', 11942),\n",
       " ('who', 11464),\n",
       " ('an', 11294),\n",
       " ('at', 11234),\n",
       " ('from', 10767),\n",
       " ('her', 10474),\n",
       " ('they', 9895),\n",
       " ('has', 9186),\n",
       " ('so', 9154),\n",
       " ('like', 9038),\n",
       " ('about', 8313),\n",
       " ('very', 8305),\n",
       " ('out', 8134),\n",
       " ('there', 8057),\n",
       " ('she', 7779),\n",
       " ('what', 7737),\n",
       " ('or', 7732)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_counts.most_common()[0:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has produces a collection of the positive and negarive words and their number of occurances. Scrolling through the data clearly shows that there is a difference between positive and negative word occurances. However straight away we can see the data is cluttered with useless words. \n",
    "\n",
    "A good step to resolve this is to find the ratio between the words occurance in the positive set and the negative set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_neg_ratios = Counter()\n",
    "\n",
    "for term, cnt in list(total_counts.most_common()):\n",
    "    if cnt > 100:\n",
    "        pos_neg_ratio = positive_counts[term] / float(negative_counts[term] + 1)\n",
    "        pos_neg_ratios[term] = pos_neg_ratio\n",
    "        \n",
    "for word, ratio in pos_neg_ratios.most_common():\n",
    "    if ratio > 1:\n",
    "        pos_neg_ratios[word] = np.log(ratio)\n",
    "    else:\n",
    "        pos_neg_ratios[word] = -np.log((1 / (ratio + 0.01)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop through each word in the total count where term is the word and cnt is the number of occurances. If the number of occurances is greater than 100 calculate the ratio between the occurances of that word in the positive list and the negative list and add it to the counter.\n",
    "\n",
    "Then, loop through all the new ratios where word is the word and ratio is the ratio calculated in the last step. If the ratio is greater than 1 it will occur more in the positive set than the negative set. Calculate the natural log to normalise. Otherwise calculate 1 / the natrural log and make it negative. the 0.01 is added so there are no divde by 0 errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('edie', 4.6913478822291435),\n",
       " ('paulie', 4.0775374439057197),\n",
       " ('felix', 3.1527360223636558),\n",
       " ('polanski', 2.8233610476132043),\n",
       " ('matthau', 2.8067217286092401),\n",
       " ('victoria', 2.6810215287142909),\n",
       " ('mildred', 2.6026896854443837),\n",
       " ('gandhi', 2.5389738710582761),\n",
       " ('flawless', 2.451005098112319),\n",
       " ('superbly', 2.2600254785752498),\n",
       " ('perfection', 2.1594842493533721),\n",
       " ('astaire', 2.1400661634962708),\n",
       " ('captures', 2.0386195471595809),\n",
       " ('voight', 2.0301704926730531),\n",
       " ('wonderfully', 2.0218960560332353),\n",
       " ('powell', 1.9783454248084671),\n",
       " ('brosnan', 1.9547990964725592),\n",
       " ('lily', 1.9203768470501485),\n",
       " ('bakshi', 1.9029851043382795),\n",
       " ('lincoln', 1.9014583864844796),\n",
       " ('refreshing', 1.8551812956655511),\n",
       " ('breathtaking', 1.8481124057791867),\n",
       " ('bourne', 1.8478489358790986),\n",
       " ('lemmon', 1.8458266904983307),\n",
       " ('delightful', 1.8002701588959635),\n",
       " ('flynn', 1.7996646487351682),\n",
       " ('andrews', 1.7764919970972666),\n",
       " ('homer', 1.7692866133759964),\n",
       " ('beautifully', 1.7626953362841438),\n",
       " ('soccer', 1.7578579175523736),\n",
       " ('elvira', 1.7397031072720019),\n",
       " ('underrated', 1.7197859696029656),\n",
       " ('gripping', 1.7165360479904674),\n",
       " ('superb', 1.7091514458966952),\n",
       " ('delight', 1.6714733033535532),\n",
       " ('welles', 1.6677068205580761),\n",
       " ('sadness', 1.663505133704376),\n",
       " ('sinatra', 1.6389967146756448),\n",
       " ('touching', 1.637217476541176),\n",
       " ('timeless', 1.62924053973028),\n",
       " ('macy', 1.6211339521972916),\n",
       " ('unforgettable', 1.6177367152487956),\n",
       " ('favorites', 1.6158688027643908),\n",
       " ('stewart', 1.6119987332957739),\n",
       " ('sullivan', 1.6094379124341003),\n",
       " ('extraordinary', 1.6094379124341003),\n",
       " ('hartley', 1.6094379124341003),\n",
       " ('brilliantly', 1.5950491749820008),\n",
       " ('friendship', 1.5677652160335325),\n",
       " ('wonderful', 1.5645425925262093)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Words most frequently seen in the postive reviews\n",
    "pos_neg_ratios.most_common()[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('boll', -4.0778152602708904),\n",
       " ('uwe', -3.9218753018711578),\n",
       " ('seagal', -3.3202501058581921),\n",
       " ('unwatchable', -3.0269848170580955),\n",
       " ('stinker', -2.9876839403711624),\n",
       " ('mst', -2.7753833211707968),\n",
       " ('incoherent', -2.7641396677532537),\n",
       " ('unfunny', -2.5545257844967644),\n",
       " ('waste', -2.4907515123361046),\n",
       " ('blah', -2.4475792789485005),\n",
       " ('horrid', -2.3715779644809971),\n",
       " ('pointless', -2.3451073877136341),\n",
       " ('atrocious', -2.3187369339642556),\n",
       " ('redeeming', -2.2667790015910296),\n",
       " ('prom', -2.2601040980178784),\n",
       " ('drivel', -2.2476029585766928),\n",
       " ('lousy', -2.2118080125207054),\n",
       " ('worst', -2.1930856334332267),\n",
       " ('laughable', -2.172468615469592),\n",
       " ('awful', -2.1385076866397488),\n",
       " ('poorly', -2.1326133844207011),\n",
       " ('wasting', -2.1178155545614512),\n",
       " ('remotely', -2.111046881095167),\n",
       " ('existent', -2.0024805005437076),\n",
       " ('boredom', -1.9241486572738005),\n",
       " ('miserably', -1.9216610938019989),\n",
       " ('sucks', -1.9166645809588516),\n",
       " ('uninspired', -1.9131499212248517),\n",
       " ('lame', -1.9117232884159072),\n",
       " ('insult', -1.9085323769376259)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Words most frequently seen in the negative reviews\n",
    "list(reversed(pos_neg_ratios.most_common()))[0:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming text into numbers\n",
    "\n",
    "The current information is usefull to those of us who can already read but not very usefull to a neural network!\n",
    "Before we train the neural network we must convert the data into a format that can be used for in the network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of different words in the data:  74074\n"
     ]
    }
   ],
   "source": [
    "vocab = set(total_counts.keys())\n",
    "vocab_size = len(vocab)\n",
    "print ('The number of different words in the data: ', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'believable',\n",
       " 'precedes',\n",
       " 'cineliterate',\n",
       " 'jubilee',\n",
       " 'seediness',\n",
       " 'grinchy',\n",
       " 'neeson',\n",
       " 'kaleidoscope',\n",
       " 'jogando',\n",
       " 'fractured',\n",
       " 'krebs',\n",
       " 'villedo',\n",
       " 'sighting',\n",
       " 'cleavers',\n",
       " 'ivanova',\n",
       " 'compositions',\n",
       " 'checkpoints',\n",
       " 'consummated',\n",
       " 'yore',\n",
       " 'penciled',\n",
       " 'hitlerian',\n",
       " 'cybil',\n",
       " 'frflutet',\n",
       " 'roseanne',\n",
       " 'evos',\n",
       " 'corral',\n",
       " 'khushi',\n",
       " 'farily',\n",
       " 'hairstyle',\n",
       " 'dreamily',\n",
       " 'hoots',\n",
       " 'miles',\n",
       " 'andersen',\n",
       " 'coverups',\n",
       " 'seraphic',\n",
       " 'listenable',\n",
       " 'maywether',\n",
       " 'thompson',\n",
       " 'byways',\n",
       " 'congratulates',\n",
       " 'probe',\n",
       " 'outwardly',\n",
       " 'heats',\n",
       " 'jaliyl',\n",
       " 'nickels',\n",
       " 'visionaries',\n",
       " 'splinters',\n",
       " 'wienberg',\n",
       " 'arsenical']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vocab)[0:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vocab is a list of all the different words found in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the input and output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "layer_0 = np.zeros((1, vocab_size))\n",
    "layer_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create the input layer with a length of the number of different words. We fill it with zeros to save memory which can improve comutiational efficiency.\n",
    "\n",
    "Next, take each word and give it a unique index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': 0,\n",
       " 'believable': 1,\n",
       " 'precedes': 2,\n",
       " 'cineliterate': 3,\n",
       " 'jubilee': 4,\n",
       " 'seediness': 5,\n",
       " 'grinchy': 6,\n",
       " 'neeson': 7,\n",
       " 'kaleidoscope': 8,\n",
       " 'jogando': 9,\n",
       " 'fractured': 10,\n",
       " 'krebs': 11,\n",
       " 'villedo': 12,\n",
       " 'sighting': 13,\n",
       " 'cleavers': 14,\n",
       " 'ivanova': 15,\n",
       " 'compositions': 16,\n",
       " 'checkpoints': 17,\n",
       " 'consummated': 18,\n",
       " 'yore': 19,\n",
       " 'penciled': 20,\n",
       " 'hitlerian': 21,\n",
       " 'cybil': 22,\n",
       " 'frflutet': 23,\n",
       " 'roseanne': 24,\n",
       " 'evos': 25,\n",
       " 'corral': 26,\n",
       " 'khushi': 27,\n",
       " 'farily': 28,\n",
       " 'hairstyle': 29,\n",
       " 'dreamily': 30,\n",
       " 'hoots': 31,\n",
       " 'miles': 32,\n",
       " 'andersen': 33,\n",
       " 'coverups': 34,\n",
       " 'seraphic': 35,\n",
       " 'listenable': 36,\n",
       " 'maywether': 37,\n",
       " 'thompson': 38,\n",
       " 'byways': 39,\n",
       " 'congratulates': 40,\n",
       " 'probe': 41,\n",
       " 'outwardly': 42,\n",
       " 'heats': 43,\n",
       " 'jaliyl': 44,\n",
       " 'nickels': 45,\n",
       " 'visionaries': 46,\n",
       " 'splinters': 47,\n",
       " 'wienberg': 48,\n",
       " 'arsenical': 49,\n",
       " 'campbell': 50,\n",
       " 'nutritional': 51,\n",
       " 'eravamo': 52,\n",
       " 'zasu': 53,\n",
       " 'beasties': 54,\n",
       " 'junk': 55,\n",
       " 'registrar': 56,\n",
       " 'milder': 57,\n",
       " 'shortland': 58,\n",
       " 'squatting': 59,\n",
       " 'gossamer': 60,\n",
       " 'practiced': 61,\n",
       " 'retentiveness': 62,\n",
       " 'dattilo': 63,\n",
       " 'telefilm': 64,\n",
       " 'unpleasantly': 65,\n",
       " 'freshman': 66,\n",
       " 'scob': 67,\n",
       " 'masssacre': 68,\n",
       " 'auds': 69,\n",
       " 'restructured': 70,\n",
       " 'eightstars': 71,\n",
       " 'souring': 72,\n",
       " 'forgery': 73,\n",
       " 'cheezily': 74,\n",
       " 'pleasures': 75,\n",
       " 'sisyphus': 76,\n",
       " 'conserving': 77,\n",
       " 'micheaux': 78,\n",
       " 'khallas': 79,\n",
       " 'dogie': 80,\n",
       " 'crappiest': 81,\n",
       " 'inhibitions': 82,\n",
       " 'dachau': 83,\n",
       " 'pastry': 84,\n",
       " 'particulars': 85,\n",
       " 'childishly': 86,\n",
       " 'piemaker': 87,\n",
       " 'garbage': 88,\n",
       " 'jewelery': 89,\n",
       " 'sipus': 90,\n",
       " 'recurrent': 91,\n",
       " 'barter': 92,\n",
       " 'yeun': 93,\n",
       " 'institutionalization': 94,\n",
       " 'intrinsically': 95,\n",
       " 'speckle': 96,\n",
       " 'adelade': 97,\n",
       " 'inheriting': 98,\n",
       " 'boarding': 99,\n",
       " 'loaded': 100,\n",
       " 'flutes': 101,\n",
       " 'deadhead': 102,\n",
       " 'pummeled': 103,\n",
       " 'thenprepare': 104,\n",
       " 'comparisons': 105,\n",
       " 'radford': 106,\n",
       " 'fairfaix': 107,\n",
       " 'littlesearch': 108,\n",
       " 'ismael': 109,\n",
       " 'mitzi': 110,\n",
       " 'elways': 111,\n",
       " 'panoply': 112,\n",
       " 'quade': 113,\n",
       " 'adhere': 114,\n",
       " 'euthanized': 115,\n",
       " 'melyvn': 116,\n",
       " 'stereotyped': 117,\n",
       " 'vastly': 118,\n",
       " 'census': 119,\n",
       " 'piled': 120,\n",
       " 'lili': 121,\n",
       " 'gamboa': 122,\n",
       " 'mwah': 123,\n",
       " 'nui': 124,\n",
       " 'climatic': 125,\n",
       " 'trickery': 126,\n",
       " 'imperative': 127,\n",
       " 'soapbox': 128,\n",
       " 'ransacking': 129,\n",
       " 'bluster': 130,\n",
       " 'jerked': 131,\n",
       " 'ifyou': 132,\n",
       " 'connived': 133,\n",
       " 'thickening': 134,\n",
       " 'gorbunov': 135,\n",
       " 'riccardo': 136,\n",
       " 'unidimensional': 137,\n",
       " 'multilevel': 138,\n",
       " 'especically': 139,\n",
       " 'dailys': 140,\n",
       " 'stairway': 141,\n",
       " 'instructors': 142,\n",
       " 'unicycle': 143,\n",
       " 'snozzcumbers': 144,\n",
       " 'exclaimed': 145,\n",
       " 'cambpell': 146,\n",
       " 'encouraged': 147,\n",
       " 'width': 148,\n",
       " 'comas': 149,\n",
       " 'leontine': 150,\n",
       " 'conelley': 151,\n",
       " 'oom': 152,\n",
       " 'mazurki': 153,\n",
       " 'exelence': 154,\n",
       " 'messick': 155,\n",
       " 'xer': 156,\n",
       " 'peer': 157,\n",
       " 'lalouche': 158,\n",
       " 'harmonized': 159,\n",
       " 'lagosi': 160,\n",
       " 'unbecomingly': 161,\n",
       " 'march': 162,\n",
       " 'lodge': 163,\n",
       " 'toughie': 164,\n",
       " 'workshop': 165,\n",
       " 'educate': 166,\n",
       " 'hairs': 167,\n",
       " 'bregovic': 168,\n",
       " 'entice': 169,\n",
       " 'woulda': 170,\n",
       " 'beffe': 171,\n",
       " 'bathouse': 172,\n",
       " 'yahoo': 173,\n",
       " 'workman': 174,\n",
       " 'padre': 175,\n",
       " 'pricks': 176,\n",
       " 'phesht': 177,\n",
       " 'metropolis': 178,\n",
       " 'nhk': 179,\n",
       " 'chattarjee': 180,\n",
       " 'stumbled': 181,\n",
       " 'spruce': 182,\n",
       " 'anachronic': 183,\n",
       " 'mauling': 184,\n",
       " 'grimms': 185,\n",
       " 'mimetic': 186,\n",
       " 'clastrophobic': 187,\n",
       " 'antiques': 188,\n",
       " 'trepidous': 189,\n",
       " 'amrohi': 190,\n",
       " 'clenches': 191,\n",
       " 'acing': 192,\n",
       " 'comptent': 193,\n",
       " 'wavered': 194,\n",
       " 'hawki': 195,\n",
       " 'luchino': 196,\n",
       " 'gutters': 197,\n",
       " 'rinsing': 198,\n",
       " 'elle': 199,\n",
       " 'misinformation': 200,\n",
       " 'alejo': 201,\n",
       " 'fille': 202,\n",
       " 'plumb': 203,\n",
       " 'jogger': 204,\n",
       " 'devalues': 205,\n",
       " 'thank': 206,\n",
       " 'lorded': 207,\n",
       " 'presents': 208,\n",
       " 'eights': 209,\n",
       " 'wan': 210,\n",
       " 'overreacting': 211,\n",
       " 'stinkers': 212,\n",
       " 'acd': 213,\n",
       " 'tragic': 214,\n",
       " 'grimaces': 215,\n",
       " 'sutra': 216,\n",
       " 'patio': 217,\n",
       " 'blossomed': 218,\n",
       " 'caro': 219,\n",
       " 'warlords': 220,\n",
       " 'isthar': 221,\n",
       " 'enjoyably': 222,\n",
       " 'porsche': 223,\n",
       " 'healey': 224,\n",
       " 'daniela': 225,\n",
       " 'agonia': 226,\n",
       " 'involution': 227,\n",
       " 'strafe': 228,\n",
       " 'olivia': 229,\n",
       " 'tenets': 230,\n",
       " 'authorty': 231,\n",
       " 'prettiest': 232,\n",
       " 'shoals': 233,\n",
       " 'hussy': 234,\n",
       " 'reiser': 235,\n",
       " 'luxembourg': 236,\n",
       " 'pickers': 237,\n",
       " 'migratory': 238,\n",
       " 'airlessness': 239,\n",
       " 'fashionthat': 240,\n",
       " 'emote': 241,\n",
       " 'rhetorics': 242,\n",
       " 'aspirations': 243,\n",
       " 'pojar': 244,\n",
       " 'fiendish': 245,\n",
       " 'dungy': 246,\n",
       " 'anywhoo': 247,\n",
       " 'floater': 248,\n",
       " 'vulgarly': 249,\n",
       " 'eddi': 250,\n",
       " 'documentarian': 251,\n",
       " 'gratingly': 252,\n",
       " 'latinity': 253,\n",
       " 'saurious': 254,\n",
       " 'nearby': 255,\n",
       " 'rickety': 256,\n",
       " 'cinema': 257,\n",
       " 'amateurs': 258,\n",
       " 'molina': 259,\n",
       " 'cartmans': 260,\n",
       " 'renn': 261,\n",
       " 'gangbangers': 262,\n",
       " 'bulked': 263,\n",
       " 'zungia': 264,\n",
       " 'remarry': 265,\n",
       " 'unison': 266,\n",
       " 'raking': 267,\n",
       " 'stepfather': 268,\n",
       " 'assign': 269,\n",
       " 'rove': 270,\n",
       " 'garlands': 271,\n",
       " 'klaws': 272,\n",
       " 'venturing': 273,\n",
       " 'escalates': 274,\n",
       " 'gwyne': 275,\n",
       " 'commands': 276,\n",
       " 'portal': 277,\n",
       " 'killshot': 278,\n",
       " 'coolio': 279,\n",
       " 'isham': 280,\n",
       " 'peahi': 281,\n",
       " 'nicola': 282,\n",
       " 'producing': 283,\n",
       " 'baja': 284,\n",
       " 'picket': 285,\n",
       " 'egging': 286,\n",
       " 'sterno': 287,\n",
       " 'chetniks': 288,\n",
       " 'yo': 289,\n",
       " 'minutia': 290,\n",
       " 'p': 291,\n",
       " 'pink': 292,\n",
       " 'sawahla': 293,\n",
       " 'janice': 294,\n",
       " 'bondy': 295,\n",
       " 'bastidge': 296,\n",
       " 'expire': 297,\n",
       " 'newscaster': 298,\n",
       " 'crewmate': 299,\n",
       " 'physician': 300,\n",
       " 'unintentional': 301,\n",
       " 'philp': 302,\n",
       " 'gingold': 303,\n",
       " 'cupertino': 304,\n",
       " 'koersk': 305,\n",
       " 'obituaries': 306,\n",
       " 'carmilla': 307,\n",
       " 'sphincters': 308,\n",
       " 'attainable': 309,\n",
       " 'sly': 310,\n",
       " 'ceaseless': 311,\n",
       " 'omnibus': 312,\n",
       " 'harline': 313,\n",
       " 'cheddar': 314,\n",
       " 'horribleness': 315,\n",
       " 'lezlie': 316,\n",
       " 'succinctly': 317,\n",
       " 'renounce': 318,\n",
       " 'yr': 319,\n",
       " 'sugimoto': 320,\n",
       " 'bracketed': 321,\n",
       " 'alexanader': 322,\n",
       " 'nineteen': 323,\n",
       " 'roofer': 324,\n",
       " 'bocabonita': 325,\n",
       " 'increses': 326,\n",
       " 'wotw': 327,\n",
       " 'daft': 328,\n",
       " 'autumn': 329,\n",
       " 'proverbs': 330,\n",
       " 'akelly': 331,\n",
       " 'corresponding': 332,\n",
       " 'ellens': 333,\n",
       " 'rattles': 334,\n",
       " 'dateless': 335,\n",
       " 'lancaster': 336,\n",
       " 'ht': 337,\n",
       " 'tobacconist': 338,\n",
       " 'ow': 339,\n",
       " 'milliagn': 340,\n",
       " 'intersected': 341,\n",
       " 'mound': 342,\n",
       " 'fantasising': 343,\n",
       " 'prescient': 344,\n",
       " 'bahgdad': 345,\n",
       " 'mangini': 346,\n",
       " 'anthropophagous': 347,\n",
       " 'structural': 348,\n",
       " 'hill': 349,\n",
       " 'habit': 350,\n",
       " 'schaeffer': 351,\n",
       " 'mailroom': 352,\n",
       " 'unknowingly': 353,\n",
       " 'greedo': 354,\n",
       " 'fascinating': 355,\n",
       " 'gainsbrough': 356,\n",
       " 'wars': 357,\n",
       " 'unfunnily': 358,\n",
       " 'apologies': 359,\n",
       " 'sauvage': 360,\n",
       " 'junebug': 361,\n",
       " 'registration': 362,\n",
       " 'beavers': 363,\n",
       " 'rei': 364,\n",
       " 'paraphrases': 365,\n",
       " 'courrieres': 366,\n",
       " 'dada': 367,\n",
       " 'andress': 368,\n",
       " 'uninformative': 369,\n",
       " 'plaudits': 370,\n",
       " 'bertie': 371,\n",
       " 'pynchon': 372,\n",
       " 'abolition': 373,\n",
       " 'preponderance': 374,\n",
       " 'congorilla': 375,\n",
       " 'cateress': 376,\n",
       " 'harilal': 377,\n",
       " 'piso': 378,\n",
       " 'quien': 379,\n",
       " 'disobeying': 380,\n",
       " 'deamon': 381,\n",
       " 'flannery': 382,\n",
       " 'dedicated': 383,\n",
       " 'merged': 384,\n",
       " 'miyoshi': 385,\n",
       " 'form': 386,\n",
       " 'picked': 387,\n",
       " 'stubborn': 388,\n",
       " 'expressiveness': 389,\n",
       " 'olivier': 390,\n",
       " 'abstract': 391,\n",
       " 'cambridge': 392,\n",
       " 'cheesiest': 393,\n",
       " 'coleen': 394,\n",
       " 'prevents': 395,\n",
       " 'josten': 396,\n",
       " 'psychomania': 397,\n",
       " 'enviably': 398,\n",
       " 'unspoken': 399,\n",
       " 'foibles': 400,\n",
       " 'illustriousness': 401,\n",
       " 'thesinger': 402,\n",
       " 'nea': 403,\n",
       " 'baloo': 404,\n",
       " 'stakeout': 405,\n",
       " 'ahhhhhh': 406,\n",
       " 'prehensile': 407,\n",
       " 'dwindle': 408,\n",
       " 'subsides': 409,\n",
       " 'nighty': 410,\n",
       " 'visualizes': 411,\n",
       " 'harass': 412,\n",
       " 'inventions': 413,\n",
       " 'compactor': 414,\n",
       " 'quibbles': 415,\n",
       " 'tantric': 416,\n",
       " 'preetam': 417,\n",
       " 'bushi': 418,\n",
       " 'splendid': 419,\n",
       " 'pallance': 420,\n",
       " 'videodisc': 421,\n",
       " 'zardkuh': 422,\n",
       " 'leaped': 423,\n",
       " 'choses': 424,\n",
       " 'kolos': 425,\n",
       " 'reintegration': 426,\n",
       " 'hatsumomo': 427,\n",
       " 'bourn': 428,\n",
       " 'touches': 429,\n",
       " 'barek': 430,\n",
       " 'specific': 431,\n",
       " 'mounties': 432,\n",
       " 'yay': 433,\n",
       " 'candlelit': 434,\n",
       " 'chulawasse': 435,\n",
       " 'provisional': 436,\n",
       " 'allegedly': 437,\n",
       " 'attach': 438,\n",
       " 'sensors': 439,\n",
       " 'distended': 440,\n",
       " 'talledega': 441,\n",
       " 'maccarthy': 442,\n",
       " 'kicking': 443,\n",
       " 'marm': 444,\n",
       " 'predeccesor': 445,\n",
       " 'minneli': 446,\n",
       " 'barraged': 447,\n",
       " 'vitametavegamin': 448,\n",
       " 'spacesuit': 449,\n",
       " 'minister': 450,\n",
       " 'ashura': 451,\n",
       " 'aghast': 452,\n",
       " 'utilize': 453,\n",
       " 'pities': 454,\n",
       " 'unaccomplished': 455,\n",
       " 'jyaada': 456,\n",
       " 'inexperience': 457,\n",
       " 'candlestick': 458,\n",
       " 'inconclusive': 459,\n",
       " 'published': 460,\n",
       " 'groot': 461,\n",
       " 'overprotective': 462,\n",
       " 'enthuses': 463,\n",
       " 'brownish': 464,\n",
       " 'bds': 465,\n",
       " 'chrissakes': 466,\n",
       " 'minded': 467,\n",
       " 'fangs': 468,\n",
       " 'tiefenbach': 469,\n",
       " 'nang': 470,\n",
       " 'hercule': 471,\n",
       " 'hofsttter': 472,\n",
       " 'mongrel': 473,\n",
       " 'islands': 474,\n",
       " 'throwaway': 475,\n",
       " 'calamai': 476,\n",
       " 'warnicki': 477,\n",
       " 'lied': 478,\n",
       " 'dismalness': 479,\n",
       " 'transplanting': 480,\n",
       " 'jarryd': 481,\n",
       " 'commit': 482,\n",
       " 'cora': 483,\n",
       " 'aryeman': 484,\n",
       " 'lowlife': 485,\n",
       " 'sunnybrook': 486,\n",
       " 'christan': 487,\n",
       " 'jest': 488,\n",
       " 'observed': 489,\n",
       " 'chronicled': 490,\n",
       " 'monstervision': 491,\n",
       " 'obsessed': 492,\n",
       " 'meanest': 493,\n",
       " 'timetable': 494,\n",
       " 'whereby': 495,\n",
       " 'suffers': 496,\n",
       " 'tomei': 497,\n",
       " 'mix': 498,\n",
       " 'submerges': 499,\n",
       " 'ganzel': 500,\n",
       " 'shekhar': 501,\n",
       " 'relocations': 502,\n",
       " 'perpetual': 503,\n",
       " 'absconded': 504,\n",
       " 'fountained': 505,\n",
       " 'occident': 506,\n",
       " 'uncountable': 507,\n",
       " 'huntingdon': 508,\n",
       " 'jeering': 509,\n",
       " 'katell': 510,\n",
       " 'szubanski': 511,\n",
       " 'speirs': 512,\n",
       " 'investors': 513,\n",
       " 'whys': 514,\n",
       " 'graceful': 515,\n",
       " 'cinematograph': 516,\n",
       " 'destructible': 517,\n",
       " 'scrat': 518,\n",
       " 'skeweredness': 519,\n",
       " 'crudest': 520,\n",
       " 'bismol': 521,\n",
       " 'perceived': 522,\n",
       " 'sheryll': 523,\n",
       " 'grift': 524,\n",
       " 'medencevic': 525,\n",
       " 'wearisome': 526,\n",
       " 'epigram': 527,\n",
       " 'transliteration': 528,\n",
       " 'liken': 529,\n",
       " 'fads': 530,\n",
       " 'trivialise': 531,\n",
       " 'guardians': 532,\n",
       " 'nav': 533,\n",
       " 'scates': 534,\n",
       " 'for': 535,\n",
       " 'migrated': 536,\n",
       " 'gentlemanlike': 537,\n",
       " 'dived': 538,\n",
       " 'hainey': 539,\n",
       " 'cashew': 540,\n",
       " 'pretzels': 541,\n",
       " 'pakistani': 542,\n",
       " 'organzation': 543,\n",
       " 'easting': 544,\n",
       " 'pseudonyms': 545,\n",
       " 'philosophers': 546,\n",
       " 'ghost': 547,\n",
       " 'kostas': 548,\n",
       " 'moto': 549,\n",
       " 'mujhe': 550,\n",
       " 'striba': 551,\n",
       " 'gentle': 552,\n",
       " 'enslaved': 553,\n",
       " 'indictment': 554,\n",
       " 'philadelphia': 555,\n",
       " 'forgiving': 556,\n",
       " 'levitating': 557,\n",
       " 'fumes': 558,\n",
       " 'luz': 559,\n",
       " 'persuade': 560,\n",
       " 'conrow': 561,\n",
       " 'surrealistic': 562,\n",
       " 'wrested': 563,\n",
       " 'subcommander': 564,\n",
       " 'notrious': 565,\n",
       " 'trojans': 566,\n",
       " 'comming': 567,\n",
       " 'jumping': 568,\n",
       " 'disabled': 569,\n",
       " 'homeboys': 570,\n",
       " 'rawail': 571,\n",
       " 'tykwer': 572,\n",
       " 'walterman': 573,\n",
       " 'meself': 574,\n",
       " 'mushed': 575,\n",
       " 'failure': 576,\n",
       " 'empahh': 577,\n",
       " 'dionna': 578,\n",
       " 'continental': 579,\n",
       " 'loudest': 580,\n",
       " 'duel': 581,\n",
       " 'terrorvision': 582,\n",
       " 'lorain': 583,\n",
       " 'chronicle': 584,\n",
       " 'grusiya': 585,\n",
       " 'dullness': 586,\n",
       " 'hitcock': 587,\n",
       " 'steinem': 588,\n",
       " 'takeout': 589,\n",
       " 'have': 590,\n",
       " 'balaji': 591,\n",
       " 'sausages': 592,\n",
       " 'ikwydls': 593,\n",
       " 'aren': 594,\n",
       " 'faludi': 595,\n",
       " 'postmortem': 596,\n",
       " 'fantasizing': 597,\n",
       " 'izes': 598,\n",
       " 'vikki': 599,\n",
       " 'grandparent': 600,\n",
       " 'snowed': 601,\n",
       " 'fixx': 602,\n",
       " 'outcomes': 603,\n",
       " 'kazaam': 604,\n",
       " 'insufficiency': 605,\n",
       " 'rydell': 606,\n",
       " 'frat': 607,\n",
       " 'substantive': 608,\n",
       " 'spinoff': 609,\n",
       " 'unbroken': 610,\n",
       " 'valve': 611,\n",
       " 'westbridbe': 612,\n",
       " 'nickelodeon': 613,\n",
       " 'mida': 614,\n",
       " 'nuns': 615,\n",
       " 'quoted': 616,\n",
       " 'arzner': 617,\n",
       " 'hagiography': 618,\n",
       " 'widower': 619,\n",
       " 'nourish': 620,\n",
       " 'cheezoid': 621,\n",
       " 'returns': 622,\n",
       " 'outfielder': 623,\n",
       " 'bushes': 624,\n",
       " 'fluctuations': 625,\n",
       " 'casnoff': 626,\n",
       " 'conflict': 627,\n",
       " 'diagnosed': 628,\n",
       " 'klebb': 629,\n",
       " 'elseavoid': 630,\n",
       " 'marching': 631,\n",
       " 'sacco': 632,\n",
       " 'smeaton': 633,\n",
       " 'louella': 634,\n",
       " 'gwynne': 635,\n",
       " 'reddish': 636,\n",
       " 'delauise': 637,\n",
       " 'flixmedia': 638,\n",
       " 'ratcatcher': 639,\n",
       " 'unengineered': 640,\n",
       " 'masturbation': 641,\n",
       " 'strenuous': 642,\n",
       " 'guayabera': 643,\n",
       " 'clubgoer': 644,\n",
       " 'wegener': 645,\n",
       " 'nanosecond': 646,\n",
       " 'aicha': 647,\n",
       " 'cradle': 648,\n",
       " 'imoogi': 649,\n",
       " 'childishness': 650,\n",
       " 'talks': 651,\n",
       " 'intransigent': 652,\n",
       " 'anticipatory': 653,\n",
       " 'skepticism': 654,\n",
       " 'gall': 655,\n",
       " 'ithaca': 656,\n",
       " 'josie': 657,\n",
       " 'cortese': 658,\n",
       " 'storymode': 659,\n",
       " 'gurney': 660,\n",
       " 'beheaded': 661,\n",
       " 'concurrently': 662,\n",
       " 'coupon': 663,\n",
       " 'traffics': 664,\n",
       " 'nightstalker': 665,\n",
       " 'midori': 666,\n",
       " 'iwaya': 667,\n",
       " 'mundanity': 668,\n",
       " 'monitored': 669,\n",
       " 'clearlly': 670,\n",
       " 'between': 671,\n",
       " 'vessela': 672,\n",
       " 'ilu': 673,\n",
       " 'fibbed': 674,\n",
       " 'tenniel': 675,\n",
       " 'stuffiness': 676,\n",
       " 'clint': 677,\n",
       " 'smartly': 678,\n",
       " 'unnamed': 679,\n",
       " 'peak': 680,\n",
       " 'giff': 681,\n",
       " 'longhair': 682,\n",
       " 'aman': 683,\n",
       " 'disgruntle': 684,\n",
       " 'wise': 685,\n",
       " 'squirted': 686,\n",
       " 'slaving': 687,\n",
       " 'muoz': 688,\n",
       " 'wrestlers': 689,\n",
       " 'sloane': 690,\n",
       " 'rawal': 691,\n",
       " 'voiceovers': 692,\n",
       " 'knarl': 693,\n",
       " 'paramedic': 694,\n",
       " 'borlenghi': 695,\n",
       " 'helge': 696,\n",
       " 'linney': 697,\n",
       " 'czarist': 698,\n",
       " 'kafka': 699,\n",
       " 'infesting': 700,\n",
       " 'wath': 701,\n",
       " 'transcendant': 702,\n",
       " 'goobacks': 703,\n",
       " 'doyeon': 704,\n",
       " 'bourgeois': 705,\n",
       " 'roget': 706,\n",
       " 'makepease': 707,\n",
       " 'lancie': 708,\n",
       " 'wellesley': 709,\n",
       " 'refusals': 710,\n",
       " 'condoning': 711,\n",
       " 'dudek': 712,\n",
       " 'bergerac': 713,\n",
       " 'sala': 714,\n",
       " 'shaka': 715,\n",
       " 'yester': 716,\n",
       " 'yumiko': 717,\n",
       " 'cooking': 718,\n",
       " 'dion': 719,\n",
       " 'benjiman': 720,\n",
       " 'packy': 721,\n",
       " 'praying': 722,\n",
       " 'outdid': 723,\n",
       " 'known': 724,\n",
       " 'goldenhersh': 725,\n",
       " 'replicas': 726,\n",
       " 'rafi': 727,\n",
       " 'gotb': 728,\n",
       " 'swathes': 729,\n",
       " 'rages': 730,\n",
       " 'emotional': 731,\n",
       " 'napton': 732,\n",
       " 'aligned': 733,\n",
       " 'squeel': 734,\n",
       " 'stimulating': 735,\n",
       " 'dearing': 736,\n",
       " 'cancer': 737,\n",
       " 'kansasi': 738,\n",
       " 'luby': 739,\n",
       " 'ste': 740,\n",
       " 'kidd': 741,\n",
       " 'palaces': 742,\n",
       " 'procreate': 743,\n",
       " 'telegraphed': 744,\n",
       " 'unify': 745,\n",
       " 'christers': 746,\n",
       " 'impaled': 747,\n",
       " 'babaloo': 748,\n",
       " 'flav': 749,\n",
       " 'imbued': 750,\n",
       " 'towed': 751,\n",
       " 'dumb': 752,\n",
       " 'heartaches': 753,\n",
       " 'equipped': 754,\n",
       " 'wyne': 755,\n",
       " 'eardrum': 756,\n",
       " 'nietzche': 757,\n",
       " 'call': 758,\n",
       " 'expects': 759,\n",
       " 'bladerunner': 760,\n",
       " 'snarl': 761,\n",
       " 'burnside': 762,\n",
       " 'likening': 763,\n",
       " 'intertextuality': 764,\n",
       " 'dreamquest': 765,\n",
       " 'bigelow': 766,\n",
       " 'puritan': 767,\n",
       " 'creds': 768,\n",
       " 'scoffs': 769,\n",
       " 'brusquely': 770,\n",
       " 'readjusts': 771,\n",
       " 'imagined': 772,\n",
       " 'involves': 773,\n",
       " 'emerge': 774,\n",
       " 'tapestries': 775,\n",
       " 'tubbs': 776,\n",
       " 'mysoju': 777,\n",
       " 'donation': 778,\n",
       " 'spokane': 779,\n",
       " 'indirection': 780,\n",
       " 'championships': 781,\n",
       " 'schotland': 782,\n",
       " 'sussanah': 783,\n",
       " 'soccer': 784,\n",
       " 'tagged': 785,\n",
       " 'greenwood': 786,\n",
       " 'travails': 787,\n",
       " 'separation': 788,\n",
       " 'pointless': 789,\n",
       " 'blazer': 790,\n",
       " 'moviehowever': 791,\n",
       " 'roaring': 792,\n",
       " 'curt': 793,\n",
       " 'billingsley': 794,\n",
       " 'withnail': 795,\n",
       " 'meatloaf': 796,\n",
       " 'pillman': 797,\n",
       " 'ching': 798,\n",
       " 'philedelphia': 799,\n",
       " 'nausicaa': 800,\n",
       " 'shipments': 801,\n",
       " 'neha': 802,\n",
       " 'nesher': 803,\n",
       " 'goldmember': 804,\n",
       " 'poet': 805,\n",
       " 'shockumenary': 806,\n",
       " 'cinderellas': 807,\n",
       " 'madnes': 808,\n",
       " 'resolve': 809,\n",
       " 'enhances': 810,\n",
       " 'stroll': 811,\n",
       " 'jogs': 812,\n",
       " 'ger': 813,\n",
       " 'liebman': 814,\n",
       " 'htm': 815,\n",
       " 'aquart': 816,\n",
       " 'freq': 817,\n",
       " 'foreveror': 818,\n",
       " 'circulating': 819,\n",
       " 'bapu': 820,\n",
       " 'soupy': 821,\n",
       " 'caesars': 822,\n",
       " 'trelkovski': 823,\n",
       " 'definative': 824,\n",
       " 'caffeine': 825,\n",
       " 'alloted': 826,\n",
       " 'writter': 827,\n",
       " 'stead': 828,\n",
       " 'daffodils': 829,\n",
       " 'mineshaft': 830,\n",
       " 'sugary': 831,\n",
       " 'invocus': 832,\n",
       " 'baguette': 833,\n",
       " 'mongkok': 834,\n",
       " 'vindicated': 835,\n",
       " 'consolidate': 836,\n",
       " 'ognianova': 837,\n",
       " 'tuff': 838,\n",
       " 'thema': 839,\n",
       " 'hulkamaniacs': 840,\n",
       " 'catlike': 841,\n",
       " 'whoppie': 842,\n",
       " 'tanga': 843,\n",
       " 'plagues': 844,\n",
       " 'purply': 845,\n",
       " 'lear': 846,\n",
       " 'accidently': 847,\n",
       " 'wrist': 848,\n",
       " 'instigators': 849,\n",
       " 'barroom': 850,\n",
       " 'ohana': 851,\n",
       " 'setups': 852,\n",
       " 'wad': 853,\n",
       " 'gaolers': 854,\n",
       " 'doorstop': 855,\n",
       " 'darwininan': 856,\n",
       " 'wwaste': 857,\n",
       " 'doesn': 858,\n",
       " 'despots': 859,\n",
       " 'appreciating': 860,\n",
       " 'thieson': 861,\n",
       " 'savor': 862,\n",
       " 'stupifyingly': 863,\n",
       " 'rollerblades': 864,\n",
       " 'stuttured': 865,\n",
       " 'artsieness': 866,\n",
       " 'scanning': 867,\n",
       " 'albas': 868,\n",
       " 'lodger': 869,\n",
       " 'mussolini': 870,\n",
       " 'saboteur': 871,\n",
       " 'stammering': 872,\n",
       " 'caries': 873,\n",
       " 'dadaist': 874,\n",
       " 'montes': 875,\n",
       " 'troi': 876,\n",
       " 'chooser': 877,\n",
       " 'creditable': 878,\n",
       " 'loutish': 879,\n",
       " 'potentialities': 880,\n",
       " 'logics': 881,\n",
       " 'differentiates': 882,\n",
       " 'ancestral': 883,\n",
       " 'rack': 884,\n",
       " 'grrr': 885,\n",
       " 'squawk': 886,\n",
       " 'ugo': 887,\n",
       " 'captains': 888,\n",
       " 'moor': 889,\n",
       " 'chador': 890,\n",
       " 'guise': 891,\n",
       " 'corpse': 892,\n",
       " 'deviates': 893,\n",
       " 'uav': 894,\n",
       " 'gloomily': 895,\n",
       " 'valga': 896,\n",
       " 'marguis': 897,\n",
       " 'valentinov': 898,\n",
       " 'berseker': 899,\n",
       " 'truer': 900,\n",
       " 'kidnapping': 901,\n",
       " 'lamebrained': 902,\n",
       " 'conjoined': 903,\n",
       " 'premade': 904,\n",
       " 'viewed': 905,\n",
       " 'steels': 906,\n",
       " 'hackery': 907,\n",
       " 'edina': 908,\n",
       " 'maitlan': 909,\n",
       " 'solino': 910,\n",
       " 'krummernes': 911,\n",
       " 'annick': 912,\n",
       " 'skinned': 913,\n",
       " 'noriaki': 914,\n",
       " 'berrymore': 915,\n",
       " 'tainos': 916,\n",
       " 'gourmet': 917,\n",
       " 'constricted': 918,\n",
       " 'natassja': 919,\n",
       " 'hsd': 920,\n",
       " 'hieroglyphs': 921,\n",
       " 'omit': 922,\n",
       " 'righted': 923,\n",
       " 'perished': 924,\n",
       " 'dza': 925,\n",
       " 'cider': 926,\n",
       " 'sailfish': 927,\n",
       " 'peaces': 928,\n",
       " 'corporate': 929,\n",
       " 'clicks': 930,\n",
       " 'bungalow': 931,\n",
       " 'flaps': 932,\n",
       " 'doughty': 933,\n",
       " 'bastions': 934,\n",
       " 'appaloosa': 935,\n",
       " 'unvarnished': 936,\n",
       " 'unsupportive': 937,\n",
       " 'dependants': 938,\n",
       " 'enigmas': 939,\n",
       " 'caster': 940,\n",
       " 'chicas': 941,\n",
       " 'deteriorate': 942,\n",
       " 'woth': 943,\n",
       " 'bufford': 944,\n",
       " 'ato': 945,\n",
       " 'nat': 946,\n",
       " 'showthe': 947,\n",
       " 'doubtfire': 948,\n",
       " 'tdd': 949,\n",
       " 'deneuve': 950,\n",
       " 'crosses': 951,\n",
       " 'threatened': 952,\n",
       " 'insulate': 953,\n",
       " 'zippier': 954,\n",
       " 'faithfully': 955,\n",
       " 'forgotten': 956,\n",
       " 'faggot': 957,\n",
       " 'argue': 958,\n",
       " 'vulgarism': 959,\n",
       " 'foppish': 960,\n",
       " 'hallelujah': 961,\n",
       " 'ortiz': 962,\n",
       " 'disarray': 963,\n",
       " 'impersonator': 964,\n",
       " 'tipping': 965,\n",
       " 'jerichow': 966,\n",
       " 'hallie': 967,\n",
       " 'kovacs': 968,\n",
       " 'hough': 969,\n",
       " 'mundanely': 970,\n",
       " 'andrenaline': 971,\n",
       " 'idaho': 972,\n",
       " 'hubba': 973,\n",
       " 'deadset': 974,\n",
       " 'vampyr': 975,\n",
       " 'verbaan': 976,\n",
       " 'luchi': 977,\n",
       " 'taratino': 978,\n",
       " 'suffice': 979,\n",
       " 'demin': 980,\n",
       " 'employes': 981,\n",
       " 'vassilis': 982,\n",
       " 'cardenas': 983,\n",
       " 'visage': 984,\n",
       " 'moviesone': 985,\n",
       " 'toyland': 986,\n",
       " 'plagiary': 987,\n",
       " 'underused': 988,\n",
       " 'espoused': 989,\n",
       " 'aloud': 990,\n",
       " 'resized': 991,\n",
       " 'ghunghroo': 992,\n",
       " 'outline': 993,\n",
       " 'stain': 994,\n",
       " 'carfare': 995,\n",
       " 'crucial': 996,\n",
       " 'expense': 997,\n",
       " 'quieted': 998,\n",
       " 'breakups': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2index = {}\n",
    "\n",
    "for i, word in enumerate(vocab):\n",
    "    word2index[word] = i\n",
    "\n",
    "word2index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to update the input layer to actually contain the information and not just zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def update_input_layer(review):\n",
    "    \n",
    "    global layer_0\n",
    "    \n",
    "    # Ensure all of layer 0 is set to 0\n",
    "    layer_0 *= 0\n",
    "    \n",
    "    for word in review.split(' '):\n",
    "        layer_0[0][word2index[word]] += 1\n",
    "        \n",
    "update_input_layer(reviews[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method takes a review and loops through each word. It then adds 1 to the value at the index of the word2index number of the word. This will build a collection of the number of word occurances at the unique index of each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 18.,   0.,   0., ...,   0.,   0.,   0.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to get the 'POSITIVE' and 'NEGATIVE' labels into a machine readable format. Here we will use a 1 or 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_target_for_label(label):\n",
    "    if label == 'POSITIVE':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'POSITIVE'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_target_for_label(labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the network\n",
    "\n",
    "For this I am going to use the NeuralNetwork class from Udacity Project 1 with some adjustments:\n",
    "- 3 Layer neural network\n",
    "- no non-linearity in the second layer (No sigmoid between layer 0 and 1)\n",
    "- use previous functions to create the training data\n",
    "- create a 'pre_process_data' function to create vocabulary for the training data and generating functions\n",
    "- modify train to train over the entire corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "\n",
    "class SentimentNetwork():\n",
    "    def __init__(self, reviews, labels, hidden_nodes = 10, learning_rate = 0.1):\n",
    "        \n",
    "        # Seed the random number generator for debugging\n",
    "        np.random.seed(1)\n",
    "        \n",
    "        self.pre_process_data(reviews, labels)\n",
    "        \n",
    "        self.init_network(self.review_vocab_size, hidden_nodes, 1, learning_rate)\n",
    "    \n",
    "    # Process all the review and label data and form unique dictionarys\n",
    "    # Give each word a unique index for entry into the network\n",
    "    def pre_process_data(self, reviews, labels):\n",
    "        # Creates a dictionary that contains one of each different word\n",
    "        review_vocab = set()\n",
    "        for review in reviews:\n",
    "            for word in review.split(' '):\n",
    "                review_vocab.add(word)\n",
    "        \n",
    "        self.review_vocab = list(review_vocab)\n",
    "        \n",
    "        # Creates a dictionary that contains one of each label type [IN our case 'POSITIVE' or 'NEGATIVE']\n",
    "        label_vocab = set()\n",
    "        for label in labels:\n",
    "            label_vocab.add(label)\n",
    "                \n",
    "        self.label_vocab = list(label_vocab)\n",
    "        \n",
    "        # Store the sizes of each dictionary\n",
    "        self.review_vocab_size = len(self.review_vocab)\n",
    "        self.label_vocab_size = len(self.label_vocab)\n",
    "        \n",
    "        # Give each word a uniquely identifying index\n",
    "        self.word2index = {}\n",
    "        for i, word in enumerate(self.review_vocab):\n",
    "            self.word2index[word] = i\n",
    "        \n",
    "        # Give each label an identifying index [in our case only 0 or 1]\n",
    "        self.label2index = {}\n",
    "        for i, label in enumerate(self.label_vocab):\n",
    "            self.label2index[label] = i\n",
    "    \n",
    "    # Initialize all the network parameters\n",
    "    def init_network(self, input_nodes, hidden_nodes, output_nodes, learning_rate):\n",
    "        # Set the number of nodes for all three layers\n",
    "        self.input_nodes = input_nodes\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.output_nodes = output_nodes\n",
    "        \n",
    "        # Initialise weights\n",
    "        self.weights_0_1 = np.zeros((self.input_nodes, self.hidden_nodes))\n",
    "        self.weights_1_2 = np.random.normal(0.0, self.output_nodes**-0.5, (self.hidden_nodes, self.output_nodes))\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.layer_0 = np.zeros((1, input_nodes))\n",
    "    \n",
    "    # Activation function\n",
    "    def sigmoid(self, x, deriv=False):\n",
    "        if deriv:\n",
    "            return x * (1 - x)\n",
    "        else:\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    # Reset the layer for the next pass and add the new words from the review\n",
    "    def update_input_layer(self, review):\n",
    "        # Clear the previous state and set to 0\n",
    "        self.layer_0 *= 0\n",
    "        \n",
    "        for word in review.split(' '):\n",
    "            if (word in self.word2index.keys()):\n",
    "                self.layer_0[0][self.word2index[word]] += 1\n",
    "    \n",
    "    # Get the neumerical reprisentation of the output\n",
    "    def get_target_for_label(self, label):\n",
    "        if label == 'POSITIVE':\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    # Train the network\n",
    "    def train(self, training_reviews, training_labels):\n",
    "        \n",
    "        # Check that the inputs match the outputs\n",
    "        assert(len(training_reviews) == len(training_labels))\n",
    "        \n",
    "        correct_so_far = 0\n",
    "        \n",
    "        # Log the start time\n",
    "        start = time.time()\n",
    "        \n",
    "        for i in range(len(training_reviews)):\n",
    "            \n",
    "            review = training_reviews[i]\n",
    "            label = training_labels[i]\n",
    "            \n",
    "            ## Forward Pass ##\n",
    "            \n",
    "            # Input Layer\n",
    "            self.update_input_layer(review)\n",
    "            \n",
    "            # Hidden Layer\n",
    "            layer_1 = self.layer_0.dot(self.weights_0_1)\n",
    "            \n",
    "            # Output layer\n",
    "            layer_2 = self.sigmoid(layer_1.dot(self.weights_1_2))\n",
    "            \n",
    "            ## Backwards Pass ##\n",
    "            \n",
    "            # Output Error\n",
    "            layer_2_error = layer_2 - self.get_target_for_label(label)\n",
    "            layer_2_delta = layer_2_error * self.sigmoid(layer_2, True)\n",
    "            \n",
    "            # Hidden Error\n",
    "            layer_1_error = layer_2_delta.dot(self.weights_1_2.T)\n",
    "            layer_1_delta = layer_1_error\n",
    "            \n",
    "            self.weights_1_2 -= layer_1.T.dot(layer_2_delta) * self.learning_rate\n",
    "            self.weights_0_1 -= self.layer_0.T.dot(layer_1_delta) * self.learning_rate\n",
    "            \n",
    "            if (np.abs(layer_2_error) < 0.5):\n",
    "                correct_so_far += 1\n",
    "            \n",
    "            reviews_per_second = i / float(time.time() - start)\n",
    "            \n",
    "            sys.stdout.write(\"\\rProgress:\" + str(100 * i/float(len(training_reviews)))[:4] + \"% Speed(reviews/sec):\" + str(reviews_per_second)[0:5] + \" #Correct:\" + str(correct_so_far) + \" #Trained:\" + str(i+1) + \" Training Accuracy:\" + str(correct_so_far * 100 / float(i+1))[:4] + \"%\")\n",
    "            if(i % 2500 == 0):\n",
    "                print(\"\")\n",
    "    \n",
    "    # Test the network\n",
    "    def test(self, testing_reviews, testing_labels):\n",
    "        \n",
    "        correct = 0\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        for i in range(len(testing_reviews)):\n",
    "            pred = self.run(testing_reviews[i])\n",
    "            \n",
    "            if pred == testing_labels[i]:\n",
    "                correct += 1\n",
    "                \n",
    "            reviews_per_second = i / float(time.time() - start)\n",
    "            \n",
    "            sys.stdout.write(\"\\rProgress:\" + str(100 * i/float(len(testing_reviews)))[:4] \\\n",
    "                             + \"% Speed(reviews/sec):\" + str(reviews_per_second)[0:5] \\\n",
    "                            + \"% #Correct:\" + str(correct) + \" #Tested:\" + str(i+1) + \" Testing Accuracy:\" + str(correct * 100 / float(i+1))[:4] + \"%\")\n",
    "    \n",
    "    # Forward propogate        \n",
    "    def run(self, review):\n",
    "        # Input Layer\n",
    "        self.update_input_layer(review.lower())\n",
    "\n",
    "        # Hidden layer\n",
    "        layer_1 = self.layer_0.dot(self.weights_0_1)\n",
    "\n",
    "        # Output layer\n",
    "        layer_2 = self.sigmoid(layer_1.dot(self.weights_1_2))\n",
    "        \n",
    "        if(layer_2[0] > 0.5):\n",
    "            return \"POSITIVE\"\n",
    "        else:\n",
    "            return \"NEGATIVE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to create an instance of the network and train it on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mlp = SentimentNetwork(reviews[:-1000],labels[:-1000], learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress:99.9% Speed(reviews/sec):915.9% #Correct:500 #Tested:1000 Testing Accuracy:50.0%"
     ]
    }
   ],
   "source": [
    "# evaluate our model before training (just to show how horrible it is)\n",
    "mlp.test(reviews[-1000:],labels[-1000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress:0.0% Speed(reviews/sec):0.0 #Correct:0 #Trained:1 Training Accuracy:0.0%\n",
      "Progress:10.4% Speed(reviews/sec):177.1 #Correct:1250 #Trained:2501 Training Accuracy:49.9%\n",
      "Progress:20.8% Speed(reviews/sec):173.9 #Correct:2500 #Trained:5001 Training Accuracy:49.9%\n",
      "Progress:31.2% Speed(reviews/sec):179.8 #Correct:3750 #Trained:7501 Training Accuracy:49.9%\n",
      "Progress:41.6% Speed(reviews/sec):182.9 #Correct:5000 #Trained:10001 Training Accuracy:49.9%\n",
      "Progress:52.0% Speed(reviews/sec):183.7 #Correct:6250 #Trained:12501 Training Accuracy:49.9%\n",
      "Progress:62.5% Speed(reviews/sec):179.2 #Correct:7500 #Trained:15001 Training Accuracy:49.9%\n",
      "Progress:72.9% Speed(reviews/sec):181.3 #Correct:8750 #Trained:17501 Training Accuracy:49.9%\n",
      "Progress:83.3% Speed(reviews/sec):183.0 #Correct:10000 #Trained:20001 Training Accuracy:49.9%\n",
      "Progress:93.7% Speed(reviews/sec):185.2 #Correct:11250 #Trained:22501 Training Accuracy:49.9%\n",
      "Progress:99.9% Speed(reviews/sec):186.3 #Correct:11999 #Trained:24000 Training Accuracy:49.9%"
     ]
    }
   ],
   "source": [
    "# train the network\n",
    "mlp.train(reviews[:-1000],labels[:-1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of training was actually worse than predicting the outcome randomly. Adjustments to the learning rate had little affect and therefore there must be something wrong with the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Noise\n",
    "\n",
    "Neural noise is when the important data is being drowned out by other pieces of unimportant data. \n",
    "\n",
    "An analagy would be:\n",
    ">the neural network is a spade and it helps you dig for gold. However no type of spade is going to help you get more gold if you are digging in the wrong place.\n",
    "\n",
    "To solve this we need to eliminate the noise in the data. First, lets check the data to see if there is anything that shouldn't be there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The function defined previously\n",
    "def update_input_layer(review):\n",
    "    \n",
    "    global layer_0\n",
    "    \n",
    "    # clear out previous state, reset the layer to be all 0s\n",
    "    layer_0 *= 0\n",
    "    for word in review.split(\" \"):\n",
    "        layer_0[0][word2index[word]] += 1\n",
    "\n",
    "update_input_layer(reviews[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 18.,   0.,   0., ...,   0.,   0.,   0.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 27),\n",
       " ('', 18),\n",
       " ('the', 9),\n",
       " ('to', 6),\n",
       " ('high', 5),\n",
       " ('i', 5),\n",
       " ('bromwell', 4),\n",
       " ('is', 4),\n",
       " ('a', 4),\n",
       " ('teachers', 4),\n",
       " ('that', 4),\n",
       " ('of', 4),\n",
       " ('it', 2),\n",
       " ('at', 2),\n",
       " ('as', 2),\n",
       " ('school', 2),\n",
       " ('my', 2),\n",
       " ('in', 2),\n",
       " ('me', 2),\n",
       " ('students', 2),\n",
       " ('their', 2),\n",
       " ('student', 2),\n",
       " ('cartoon', 1),\n",
       " ('comedy', 1),\n",
       " ('ran', 1),\n",
       " ('same', 1),\n",
       " ('time', 1),\n",
       " ('some', 1),\n",
       " ('other', 1),\n",
       " ('programs', 1),\n",
       " ('about', 1),\n",
       " ('life', 1),\n",
       " ('such', 1),\n",
       " ('years', 1),\n",
       " ('teaching', 1),\n",
       " ('profession', 1),\n",
       " ('lead', 1),\n",
       " ('believe', 1),\n",
       " ('s', 1),\n",
       " ('satire', 1),\n",
       " ('much', 1),\n",
       " ('closer', 1),\n",
       " ('reality', 1),\n",
       " ('than', 1),\n",
       " ('scramble', 1),\n",
       " ('survive', 1),\n",
       " ('financially', 1),\n",
       " ('insightful', 1),\n",
       " ('who', 1),\n",
       " ('can', 1)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_counter = Counter()\n",
    "\n",
    "for word in reviews[0].split(' '):\n",
    "    review_counter[word] += 1\n",
    "    \n",
    "review_counter.most_common()[0:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the data we can see there are 18 instances of an empty space. As this does not convey the sentiment in any way it must be removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing noise in the input data\n",
    "\n",
    "In this situation we can see that the data is being scewed by the large number of useless values. We can resolve this by changing the update_input_layer method to not incriment the word count but just log it as present!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "\n",
    "class SentimentNetwork():\n",
    "    def __init__(self, reviews, labels, hidden_nodes = 10, learning_rate = 0.1):\n",
    "        \n",
    "        # Seed the random number generator for debugging\n",
    "        np.random.seed(1)\n",
    "        \n",
    "        self.pre_process_data(reviews, labels)\n",
    "        \n",
    "        self.init_network(self.review_vocab_size, hidden_nodes, 1, learning_rate)\n",
    "    \n",
    "    # Process all the review and label data and form unique dictionarys\n",
    "    # Give each word a unique index for entry into the network\n",
    "    def pre_process_data(self, reviews, labels):\n",
    "        # Creates a dictionary that contains one of each different word\n",
    "        review_vocab = set()\n",
    "        for review in reviews:\n",
    "            for word in review.split(' '):\n",
    "                review_vocab.add(word)\n",
    "        \n",
    "        self.review_vocab = list(review_vocab)\n",
    "        \n",
    "        # Creates a dictionary that contains one of each label type [IN our case 'POSITIVE' or 'NEGATIVE']\n",
    "        label_vocab = set()\n",
    "        for label in labels:\n",
    "            label_vocab.add(label)\n",
    "                \n",
    "        self.label_vocab = list(label_vocab)\n",
    "        \n",
    "        # Store the sizes of each dictionary\n",
    "        self.review_vocab_size = len(self.review_vocab)\n",
    "        self.label_vocab_size = len(self.label_vocab)\n",
    "        \n",
    "        # Give each word a uniquely identifying index\n",
    "        self.word2index = {}\n",
    "        for i, word in enumerate(self.review_vocab):\n",
    "            self.word2index[word] = i\n",
    "        \n",
    "        # Give each label an identifying index [in our case only 0 or 1]\n",
    "        self.label2index = {}\n",
    "        for i, label in enumerate(self.label_vocab):\n",
    "            self.label2index[label] = i\n",
    "    \n",
    "    # Initialize all the network parameters\n",
    "    def init_network(self, input_nodes, hidden_nodes, output_nodes, learning_rate):\n",
    "        # Set the number of nodes for all three layers\n",
    "        self.input_nodes = input_nodes\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.output_nodes = output_nodes\n",
    "        \n",
    "        # Initialise weights\n",
    "        self.weights_0_1 = np.zeros((self.input_nodes, self.hidden_nodes))\n",
    "        self.weights_1_2 = np.random.normal(0.0, self.output_nodes**-0.5, (self.hidden_nodes, self.output_nodes))\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.layer_0 = np.zeros((1, input_nodes))\n",
    "    \n",
    "    # Activation function\n",
    "    def sigmoid(self, x, deriv=False):\n",
    "        if deriv:\n",
    "            return x * (1 - x)\n",
    "        else:\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    # Reset the layer for the next pass and add the new words from the review\n",
    "    def update_input_layer(self, review):\n",
    "        # Clear the previous state and set to 0\n",
    "        self.layer_0 *= 0\n",
    "        \n",
    "        for word in review.split(' '):\n",
    "            if (word in self.word2index.keys()):\n",
    "                self.layer_0[0][self.word2index[word]] = 1\n",
    "    \n",
    "    # Get the neumerical reprisentation of the output\n",
    "    def get_target_for_label(self, label):\n",
    "        if label == 'POSITIVE':\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    # Train the network\n",
    "    def train(self, training_reviews, training_labels):\n",
    "        \n",
    "        # Check that the inputs match the outputs\n",
    "        assert(len(training_reviews) == len(training_labels))\n",
    "        \n",
    "        correct_so_far = 0\n",
    "        \n",
    "        # Log the start time\n",
    "        start = time.time()\n",
    "        \n",
    "        for i in range(len(training_reviews)):\n",
    "            \n",
    "            review = training_reviews[i]\n",
    "            label = training_labels[i]\n",
    "            \n",
    "            ## Forward Pass ##\n",
    "            \n",
    "            # Input Layer\n",
    "            self.update_input_layer(review)\n",
    "            \n",
    "            # Hidden Layer\n",
    "            layer_1 = self.layer_0.dot(self.weights_0_1)\n",
    "            \n",
    "            # Output layer\n",
    "            layer_2 = self.sigmoid(layer_1.dot(self.weights_1_2))\n",
    "            \n",
    "            ## Backwards Pass ##\n",
    "            \n",
    "            # Output Error\n",
    "            layer_2_error = layer_2 - self.get_target_for_label(label)\n",
    "            layer_2_delta = layer_2_error * self.sigmoid(layer_2, True)\n",
    "            \n",
    "            # Hidden Error\n",
    "            layer_1_error = layer_2_delta.dot(self.weights_1_2.T)\n",
    "            layer_1_delta = layer_1_error\n",
    "            \n",
    "            self.weights_1_2 -= layer_1.T.dot(layer_2_delta) * self.learning_rate\n",
    "            self.weights_0_1 -= self.layer_0.T.dot(layer_1_delta) * self.learning_rate\n",
    "            \n",
    "            if (np.abs(layer_2_error) < 0.5):\n",
    "                correct_so_far += 1\n",
    "            \n",
    "            reviews_per_second = i / float(time.time() - start)\n",
    "            \n",
    "            sys.stdout.write(\"\\rProgress:\" + str(100 * i/float(len(training_reviews)))[:4] + \"% Speed(reviews/sec):\" + str(reviews_per_second)[0:5] + \" #Correct:\" + str(correct_so_far) + \" #Trained:\" + str(i+1) + \" Training Accuracy:\" + str(correct_so_far * 100 / float(i+1))[:4] + \"%\")\n",
    "            if(i % 2500 == 0):\n",
    "                print(\"\")\n",
    "    \n",
    "    # Test the network\n",
    "    def test(self, testing_reviews, testing_labels):\n",
    "        \n",
    "        correct = 0\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        for i in range(len(testing_reviews)):\n",
    "            pred = self.run(testing_reviews[i])\n",
    "            \n",
    "            if pred == testing_labels[i]:\n",
    "                correct += 1\n",
    "                \n",
    "            reviews_per_second = i / float(time.time() - start)\n",
    "            \n",
    "            sys.stdout.write(\"\\rProgress:\" + str(100 * i/float(len(testing_reviews)))[:4] \\\n",
    "                             + \"% Speed(reviews/sec):\" + str(reviews_per_second)[0:5] \\\n",
    "                            + \"% #Correct:\" + str(correct) + \" #Tested:\" + str(i+1) + \" Testing Accuracy:\" + str(correct * 100 / float(i+1))[:4] + \"%\")\n",
    "    \n",
    "    # Forward propogate        \n",
    "    def run(self, review):\n",
    "        # Input Layer\n",
    "        self.update_input_layer(review.lower())\n",
    "\n",
    "        # Hidden layer\n",
    "        layer_1 = self.layer_0.dot(self.weights_0_1)\n",
    "\n",
    "        # Output layer\n",
    "        layer_2 = self.sigmoid(layer_1.dot(self.weights_1_2))\n",
    "        \n",
    "        if(layer_2[0] > 0.5):\n",
    "            return \"POSITIVE\"\n",
    "        else:\n",
    "            return \"NEGATIVE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp = SentimentNetwork(reviews[:-1000],labels[:-1000], learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress:0.0% Speed(reviews/sec):0.0 #Correct:0 #Trained:1 Training Accuracy:0.0%\n",
      "Progress:10.4% Speed(reviews/sec):181.3 #Correct:1823 #Trained:2501 Training Accuracy:72.8%\n",
      "Progress:20.8% Speed(reviews/sec):193.3 #Correct:3798 #Trained:5001 Training Accuracy:75.9%\n",
      "Progress:31.2% Speed(reviews/sec):198.1 #Correct:5877 #Trained:7501 Training Accuracy:78.3%\n",
      "Progress:41.6% Speed(reviews/sec):200.7 #Correct:8019 #Trained:10001 Training Accuracy:80.1%\n",
      "Progress:52.0% Speed(reviews/sec):200.9 #Correct:10142 #Trained:12501 Training Accuracy:81.1%\n",
      "Progress:62.5% Speed(reviews/sec):200.8 #Correct:12279 #Trained:15001 Training Accuracy:81.8%\n",
      "Progress:72.9% Speed(reviews/sec):201.9 #Correct:14394 #Trained:17501 Training Accuracy:82.2%\n",
      "Progress:83.3% Speed(reviews/sec):202.4 #Correct:16565 #Trained:20001 Training Accuracy:82.8%\n",
      "Progress:93.7% Speed(reviews/sec):203.0 #Correct:18750 #Trained:22501 Training Accuracy:83.3%\n",
      "Progress:99.9% Speed(reviews/sec):203.3 #Correct:20070 #Trained:24000 Training Accuracy:83.6%"
     ]
    }
   ],
   "source": [
    "mlp.train(reviews[:-1000],labels[:-1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantly we can see that the network is now training successfully; attaining an accuracy upwards of 83.5%!\n",
    "\n",
    "However the training process is still slow and we need to find remove inefficiencies from the code to make it run faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress:99.9% Speed(reviews/sec):1081.% #Correct:848 #Tested:1000 Testing Accuracy:84.8%"
     ]
    }
   ],
   "source": [
    "mlp.test(reviews[-1000:],labels[-1000:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing inefficiencies in the network\n",
    "\n",
    "Now we need to identify things that can slow down the network. Lets start by looking at layer_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 18.,   0.,   0., ...,   0.,   0.,   0.]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that layer_0 is the length of the entire dictionary. However most of those inputs will simply be zero. Considering 0 * any number is always 0 it is a wasted calculation. Looking at a smaller example we can see..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_0 = np.zeros(10)\n",
    "\n",
    "layer_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  1.])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_0[4] = 1\n",
    "layer_0[9] = 1\n",
    "\n",
    "layer_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.10503756,  0.44222989,  0.24392938, -0.55961832,  0.21389503])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_0_1 = np.random.randn(10, 5)\n",
    "layer_0.dot(weights_0_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On its own this doesnt look like a problem but lets see how we can get the same results with much less computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.10503756,  0.44222989,  0.24392938, -0.55961832,  0.21389503])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = [4,9]\n",
    "layer_1 = np.zeros(5)\n",
    "for index in indices:\n",
    "    layer_1 += (weights_0_1[index])\n",
    "    \n",
    "layer_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how this produces exactly the same results because 0 is a wasted value! It should also be noted that the input can only be a zero or a 1. 1 * x is always equal to x so the first layer computation can be significantly reduced!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing inefficiencies in the network\n",
    "\n",
    "Now that a serious inefficiency has been found in the network we need to actually adapt the network to handle it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "\n",
    "# Let's tweak the network from before to model these phenomena\n",
    "class SentimentNetwork:\n",
    "    def __init__(self, reviews,labels,hidden_nodes = 10, learning_rate = 0.1):\n",
    "       \n",
    "        np.random.seed(1)\n",
    "    \n",
    "        self.pre_process_data(reviews)\n",
    "        \n",
    "        self.init_network(len(self.review_vocab),hidden_nodes, 1, learning_rate)\n",
    "        \n",
    "        \n",
    "    def pre_process_data(self,reviews):\n",
    "        \n",
    "        review_vocab = set()\n",
    "        for review in reviews:\n",
    "            for word in review.split(\" \"):\n",
    "                review_vocab.add(word)\n",
    "        self.review_vocab = list(review_vocab)\n",
    "        \n",
    "        label_vocab = set()\n",
    "        for label in labels:\n",
    "            label_vocab.add(label)\n",
    "        \n",
    "        self.label_vocab = list(label_vocab)\n",
    "        \n",
    "        self.review_vocab_size = len(self.review_vocab)\n",
    "        self.label_vocab_size = len(self.label_vocab)\n",
    "        \n",
    "        self.word2index = {}\n",
    "        for i, word in enumerate(self.review_vocab):\n",
    "            self.word2index[word] = i\n",
    "        \n",
    "        self.label2index = {}\n",
    "        for i, label in enumerate(self.label_vocab):\n",
    "            self.label2index[label] = i\n",
    "         \n",
    "        \n",
    "    def init_network(self, input_nodes, hidden_nodes, output_nodes, learning_rate):\n",
    "        # Set number of nodes in input, hidden and output layers.\n",
    "        self.input_nodes = input_nodes\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.output_nodes = output_nodes\n",
    "\n",
    "        # Initialize weights\n",
    "        self.weights_0_1 = np.zeros((self.input_nodes,self.hidden_nodes))\n",
    "    \n",
    "        self.weights_1_2 = np.random.normal(0.0, self.output_nodes**-0.5, \n",
    "                                                (self.hidden_nodes, self.output_nodes))\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.layer_0 = np.zeros((1,input_nodes))\n",
    "        self.layer_1 = np.zeros((1,hidden_nodes))\n",
    "        \n",
    "    def sigmoid(self,x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    \n",
    "    def sigmoid_output_2_derivative(self,output):\n",
    "        return output * (1 - output)\n",
    "    \n",
    "    def update_input_layer(self,review):\n",
    "\n",
    "        # clear out previous state, reset the layer to be all 0s\n",
    "        self.layer_0 *= 0\n",
    "        for word in review.split(\" \"):\n",
    "            self.layer_0[0][self.word2index[word]] = 1\n",
    "\n",
    "    def get_target_for_label(self,label):\n",
    "        if(label == 'POSITIVE'):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def train(self, training_reviews_raw, training_labels):\n",
    "        \n",
    "        training_reviews = list()\n",
    "        for review in training_reviews_raw:\n",
    "            indices = set()\n",
    "            for word in review.split(\" \"):\n",
    "                if(word in self.word2index.keys()):\n",
    "                    indices.add(self.word2index[word])\n",
    "            training_reviews.append(list(indices))\n",
    "        \n",
    "        assert(len(training_reviews) == len(training_labels))\n",
    "        \n",
    "        correct_so_far = 0\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        for i in range(len(training_reviews)):\n",
    "            \n",
    "            review = training_reviews[i]\n",
    "            label = training_labels[i]\n",
    "            \n",
    "            #### Implement the forward pass here ####\n",
    "            ### Forward pass ###\n",
    "\n",
    "            # Input Layer\n",
    "\n",
    "            # Hidden layer\n",
    "#             layer_1 = self.layer_0.dot(self.weights_0_1)\n",
    "            self.layer_1 *= 0\n",
    "            for index in review:\n",
    "                self.layer_1 += self.weights_0_1[index]\n",
    "            \n",
    "            # Output layer\n",
    "            layer_2 = self.sigmoid(self.layer_1.dot(self.weights_1_2))\n",
    "\n",
    "            #### Implement the backward pass here ####\n",
    "            ### Backward pass ###\n",
    "\n",
    "            # Output error\n",
    "            layer_2_error = layer_2 - self.get_target_for_label(label) # Output layer error is the difference between desired target and actual output.\n",
    "            layer_2_delta = layer_2_error * self.sigmoid_output_2_derivative(layer_2)\n",
    "\n",
    "            # Backpropagated error\n",
    "            layer_1_error = layer_2_delta.dot(self.weights_1_2.T) # errors propagated to the hidden layer\n",
    "            layer_1_delta = layer_1_error # hidden layer gradients - no nonlinearity so it's the same as the error\n",
    "\n",
    "            # Update the weights\n",
    "            self.weights_1_2 -= self.layer_1.T.dot(layer_2_delta) * self.learning_rate # update hidden-to-output weights with gradient descent step\n",
    "            \n",
    "            for index in review:\n",
    "                self.weights_0_1[index] -= layer_1_delta[0] * self.learning_rate # update input-to-hidden weights with gradient descent step\n",
    "\n",
    "            if(np.abs(layer_2_error) < 0.5):\n",
    "                correct_so_far += 1\n",
    "            \n",
    "            reviews_per_second = i / float(time.time() - start)\n",
    "            \n",
    "            sys.stdout.write(\"\\rProgress:\" + str(100 * i/float(len(training_reviews)))[:4] + \"% Speed(reviews/sec):\" + str(reviews_per_second)[0:5] + \" #Correct:\" + str(correct_so_far) + \" #Trained:\" + str(i+1) + \" Training Accuracy:\" + str(correct_so_far * 100 / float(i+1))[:4] + \"%\")\n",
    "        \n",
    "    \n",
    "    def test(self, testing_reviews, testing_labels):\n",
    "        \n",
    "        correct = 0\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        for i in range(len(testing_reviews)):\n",
    "            pred = self.run(testing_reviews[i])\n",
    "            if(pred == testing_labels[i]):\n",
    "                correct += 1\n",
    "            \n",
    "            reviews_per_second = i / float(time.time() - start)\n",
    "            \n",
    "            sys.stdout.write(\"\\rProgress:\" + str(100 * i/float(len(testing_reviews)))[:4] \\\n",
    "                             + \"% Speed(reviews/sec):\" + str(reviews_per_second)[0:5] \\\n",
    "                            + \"% #Correct:\" + str(correct) + \" #Tested:\" + str(i+1) + \" Testing Accuracy:\" + str(correct * 100 / float(i+1))[:4] + \"%\")\n",
    "    \n",
    "    def run(self, review):\n",
    "        \n",
    "        # Input Layer\n",
    "\n",
    "\n",
    "        # Hidden layer\n",
    "        self.layer_1 *= 0\n",
    "        unique_indices = set()\n",
    "        for word in review.lower().split(\" \"):\n",
    "            if word in self.word2index.keys():\n",
    "                unique_indices.add(self.word2index[word])\n",
    "        for index in unique_indices:\n",
    "            self.layer_1 += self.weights_0_1[index]\n",
    "        \n",
    "        # Output layer\n",
    "        layer_2 = self.sigmoid(self.layer_1.dot(self.weights_1_2))\n",
    "        \n",
    "        if(layer_2[0] > 0.5):\n",
    "            return \"POSITIVE\"\n",
    "        else:\n",
    "            return \"NEGATIVE\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp = SentimentNetwork(reviews[:-1000],labels[:-1000], learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress:99.9% Speed(reviews/sec):1998. #Correct:20099 #Trained:24000 Training Accuracy:83.7%"
     ]
    }
   ],
   "source": [
    "mlp.train(reviews[:-1000],labels[:-1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress:99.9% Speed(reviews/sec):2020.% #Correct:852 #Tested:1000 Testing Accuracy:85.2%"
     ]
    }
   ],
   "source": [
    "mlp.test(reviews[-1000:],labels[-1000:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Further Noise Reduction\n",
    "\n",
    "Lets continue to reduce the amount of noise to make training even more accurate! Small changes can make a huge impact on how fast the network trains.\n",
    "\n",
    "Here we are going to look at how to seperate out the key words more effectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('edie', 4.6913478822291435),\n",
       " ('paulie', 4.0775374439057197),\n",
       " ('felix', 3.1527360223636558),\n",
       " ('polanski', 2.8233610476132043),\n",
       " ('matthau', 2.8067217286092401),\n",
       " ('victoria', 2.6810215287142909),\n",
       " ('mildred', 2.6026896854443837),\n",
       " ('gandhi', 2.5389738710582761),\n",
       " ('flawless', 2.451005098112319),\n",
       " ('superbly', 2.2600254785752498),\n",
       " ('perfection', 2.1594842493533721),\n",
       " ('astaire', 2.1400661634962708),\n",
       " ('captures', 2.0386195471595809),\n",
       " ('voight', 2.0301704926730531),\n",
       " ('wonderfully', 2.0218960560332353),\n",
       " ('powell', 1.9783454248084671),\n",
       " ('brosnan', 1.9547990964725592),\n",
       " ('lily', 1.9203768470501485),\n",
       " ('bakshi', 1.9029851043382795),\n",
       " ('lincoln', 1.9014583864844796),\n",
       " ('refreshing', 1.8551812956655511),\n",
       " ('breathtaking', 1.8481124057791867),\n",
       " ('bourne', 1.8478489358790986),\n",
       " ('lemmon', 1.8458266904983307),\n",
       " ('delightful', 1.8002701588959635),\n",
       " ('flynn', 1.7996646487351682),\n",
       " ('andrews', 1.7764919970972666),\n",
       " ('homer', 1.7692866133759964),\n",
       " ('beautifully', 1.7626953362841438),\n",
       " ('soccer', 1.7578579175523736)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_neg_ratios.most_common()[0:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data being passed at the moment is full of words we can use for sentiment analysis. However it also contains many words that are not usefull to us. There are many names that occur commonly that don't really express sentiment. The next step is to remove those useless words.\n",
    "\n",
    "A good way to do this is to use bokeh; It is a data visualisation library, which will allow us to see what is going on with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"http://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"fb46ab6e-4b43-42b8-8be9-d01fcb3c34cc\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(global) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof (window._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "    window._bokeh_onload_callbacks = [];\n",
       "    window._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "\n",
       "  \n",
       "  if (typeof (window._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    window._bokeh_timeout = Date.now() + 5000;\n",
       "    window._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    if (window.Bokeh !== undefined) {\n",
       "      document.getElementById(\"fb46ab6e-4b43-42b8-8be9-d01fcb3c34cc\").textContent = \"BokehJS successfully loaded.\";\n",
       "    } else if (Date.now() < window._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    window._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    delete window._bokeh_onload_callbacks\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    window._bokeh_onload_callbacks.push(callback);\n",
       "    if (window._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    window._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        window._bokeh_is_loading--;\n",
       "        if (window._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"fb46ab6e-4b43-42b8-8be9-d01fcb3c34cc\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid 'fb46ab6e-4b43-42b8-8be9-d01fcb3c34cc' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.4.min.js\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "      document.getElementById(\"fb46ab6e-4b43-42b8-8be9-d01fcb3c34cc\").textContent = \"BokehJS is loading...\";\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.4.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.4.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.4.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.4.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((window.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i](window.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < window._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!window._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      window._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"fb46ab6e-4b43-42b8-8be9-d01fcb3c34cc\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (window._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(this));"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from bokeh.models import ColumnDataSource, LabelSet\n",
    "from bokeh.plotting import figure, show, output_file\n",
    "from bokeh.io import output_notebook\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <div class=\"bk-plotdiv\" id=\"cc88d3ef-17a0-4a9d-85fe-8eb4aa4f0334\"></div>\n",
       "    </div>\n",
       "<script type=\"text/javascript\">\n",
       "  \n",
       "  (function(global) {\n",
       "    function now() {\n",
       "      return new Date();\n",
       "    }\n",
       "  \n",
       "    var force = false;\n",
       "  \n",
       "    if (typeof (window._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "      window._bokeh_onload_callbacks = [];\n",
       "      window._bokeh_is_loading = undefined;\n",
       "    }\n",
       "  \n",
       "  \n",
       "    \n",
       "    if (typeof (window._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "      window._bokeh_timeout = Date.now() + 0;\n",
       "      window._bokeh_failed_load = false;\n",
       "    }\n",
       "  \n",
       "    var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "       \"<div style='background-color: #fdd'>\\n\"+\n",
       "       \"<p>\\n\"+\n",
       "       \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "       \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "       \"</p>\\n\"+\n",
       "       \"<ul>\\n\"+\n",
       "       \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "       \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "       \"</ul>\\n\"+\n",
       "       \"<code>\\n\"+\n",
       "       \"from bokeh.resources import INLINE\\n\"+\n",
       "       \"output_notebook(resources=INLINE)\\n\"+\n",
       "       \"</code>\\n\"+\n",
       "       \"</div>\"}};\n",
       "  \n",
       "    function display_loaded() {\n",
       "      if (window.Bokeh !== undefined) {\n",
       "        document.getElementById(\"cc88d3ef-17a0-4a9d-85fe-8eb4aa4f0334\").textContent = \"BokehJS successfully loaded.\";\n",
       "      } else if (Date.now() < window._bokeh_timeout) {\n",
       "        setTimeout(display_loaded, 100)\n",
       "      }\n",
       "    }\n",
       "  \n",
       "    function run_callbacks() {\n",
       "      window._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "      delete window._bokeh_onload_callbacks\n",
       "      console.info(\"Bokeh: all callbacks have finished\");\n",
       "    }\n",
       "  \n",
       "    function load_libs(js_urls, callback) {\n",
       "      window._bokeh_onload_callbacks.push(callback);\n",
       "      if (window._bokeh_is_loading > 0) {\n",
       "        console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "        return null;\n",
       "      }\n",
       "      if (js_urls == null || js_urls.length === 0) {\n",
       "        run_callbacks();\n",
       "        return null;\n",
       "      }\n",
       "      console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "      window._bokeh_is_loading = js_urls.length;\n",
       "      for (var i = 0; i < js_urls.length; i++) {\n",
       "        var url = js_urls[i];\n",
       "        var s = document.createElement('script');\n",
       "        s.src = url;\n",
       "        s.async = false;\n",
       "        s.onreadystatechange = s.onload = function() {\n",
       "          window._bokeh_is_loading--;\n",
       "          if (window._bokeh_is_loading === 0) {\n",
       "            console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "            run_callbacks()\n",
       "          }\n",
       "        };\n",
       "        s.onerror = function() {\n",
       "          console.warn(\"failed to load library \" + url);\n",
       "        };\n",
       "        console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      }\n",
       "    };var element = document.getElementById(\"cc88d3ef-17a0-4a9d-85fe-8eb4aa4f0334\");\n",
       "    if (element == null) {\n",
       "      console.log(\"Bokeh: ERROR: autoload.js configured with elementid 'cc88d3ef-17a0-4a9d-85fe-8eb4aa4f0334' but no matching script tag was found. \")\n",
       "      return false;\n",
       "    }\n",
       "  \n",
       "    var js_urls = [];\n",
       "  \n",
       "    var inline_js = [\n",
       "      function(Bokeh) {\n",
       "        (function() {\n",
       "          var fn = function() {\n",
       "            var docs_json = {\"7e445b23-4bc0-4dfa-b6c8-08386bc4d04c\":{\"roots\":{\"references\":[{\"attributes\":{\"formatter\":{\"id\":\"74e7aa9b-1532-4b13-8aa6-9f97d4963a86\",\"type\":\"BasicTickFormatter\"},\"plot\":{\"id\":\"3d3a0acb-b2f1-48fd-900f-0c45eae16dbc\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"b943d457-21c1-4b5e-a0dd-9c62ed3adf59\",\"type\":\"BasicTicker\"}},\"id\":\"26861075-037c-412f-8379-1d74034aaa9e\",\"type\":\"LinearAxis\"},{\"attributes\":{\"data_source\":{\"id\":\"5c90751c-1793-49bb-8456-c22501b19054\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"880c3728-634d-4b36-b0af-32c19ef9bd2f\",\"type\":\"Quad\"},\"hover_glyph\":null,\"nonselection_glyph\":{\"id\":\"a02f0734-eb22-4920-828c-b5717ecf8fae\",\"type\":\"Quad\"},\"selection_glyph\":null},\"id\":\"f5b629c3-dbd2-4d6b-bae8-d3cfe3ac261f\",\"type\":\"GlyphRenderer\"},{\"attributes\":{},\"id\":\"b943d457-21c1-4b5e-a0dd-9c62ed3adf59\",\"type\":\"BasicTicker\"},{\"attributes\":{\"callback\":null,\"column_names\":[\"left\",\"right\",\"top\"],\"data\":{\"left\":{\"__ndarray__\":\"Cvm3za5PEMCZHufvxesPwB5LXkQuOA/AonfVmJaEDsAnpEzt/tANwKzQw0FnHQ3AMf06ls9pDMC2KbLqN7YLwDpWKT+gAgvAv4KgkwhPCsBErxfocJsJwMnbjjzZ5wjATggGkUE0CMDSNH3lqYAHwFhh9DkSzQbA3I1rjnoZBsBhuuLi4mUFwObmWTdLsgTAahPRi7P+A8DwP0jgG0sDwHRsvzSElwLA+Zg2iezjAcB+xa3dVDABwAPyJDK9fADAED04DUuS/78Ylia2Gyv+vyLvFF/sw/y/LEgDCL1c+782ofGwjfX5v0D631lejvi/SFPOAi8n979SrLyr/7/1v1wFq1TQWPS/Zl6Z/aDx8r9wt4emcYrxv3gQdk9CI/C/BNPI8CV47b8YhaVCx6nqvyw3gpRo2+e/QOle5gkN5b9Qmzs4qz7iv8iaMBSZ4N6/8P7pt9tD2b8YY6NbHqfTv4COuf7BFMy/0FYsRkfbwL+AfHw2Moemv4BiuKu4XqY/QFB74yjRwD8AiAicowrMP+DfSioPotM/sHuRhsw+2T+QF9jiidveP7BZj58jPOI/oKeyTYIK5T+Q9dX74NjnP3hD+ak/p+o/aJEcWJ517T+o7x+D/iHwP6CWMdotifE/mD1DMV3w8j+M5FSIjFf0P4SLZt+7vvU/eDJ4Nusl9z9w2YmNGo34P2iAm+RJ9Pk/XCetO3lb+z9Uzr6SqML8P0h10OnXKf4/QBziQAeR/z+c4flLG3wAQBa1gveyLwFAkogLo0rjAUAMXJRO4pYCQIgvHfp5SgNABAOmpRH+A0B+1i5RqbEEQPqpt/xAZQVAdH1AqNgYBkDwUMlTcMwGQGwkUv8HgAdA5vfaqp8zCEBiy2NWN+cIQNye7AHPmglAWHJ1rWZOCkDSRf5Y/gELQE4ZhwSWtQtAyuwPsC1pDEBEwJhbxRwNQMCTIQdd0A1AOmeqsvSDDkC2OjNejDcPQDAOvAkk6w9A1nCi2l1PEECU2mawKakQQFJEK4b1AhFADq7vW8FcEUDMF7QxjbYRQIqBeAdZEBJASOs83SRqEkA=\",\"dtype\":\"float64\",\"shape\":[100]},\"right\":{\"__ndarray__\":\"mR7n78XrD8AeS15ELjgPwKJ31ZiWhA7AJ6RM7f7QDcCs0MNBZx0NwDH9OpbPaQzAtimy6je2C8A6Vik/oAILwL+CoJMITwrARK8X6HCbCcDJ24482ecIwE4IBpFBNAjA0jR95amAB8BYYfQ5Es0GwNyNa456GQbAYbri4uJlBcDm5lk3S7IEwGoT0Yuz/gPA8D9I4BtLA8B0bL80hJcCwPmYNons4wHAfsWt3VQwAcAD8iQyvXwAwBA9OA1Lkv+/GJYmthsr/r8i7xRf7MP8vyxIAwi9XPu/NqHxsI31+b9A+t9ZXo74v0hTzgIvJ/e/Uqy8q/+/9b9cBatU0Fj0v2Zemf2g8fK/cLeHpnGK8b94EHZPQiPwvwTTyPAleO2/GIWlQsep6r8sN4KUaNvnv0DpXuYJDeW/UJs7OKs+4r/ImjAUmeDev/D+6bfbQ9m/GGOjWx6n07+Ajrn+wRTMv9BWLEZH28C/gHx8NjKHpr+AYriruF6mP0BQe+Mo0cA/AIgInKMKzD/g30oqD6LTP7B7kYbMPtk/kBfY4onb3j+wWY+fIzziP6Cnsk2CCuU/kPXV++DY5z94Q/mpP6fqP2iRHFiede0/qO8fg/4h8D+gljHaLYnxP5g9QzFd8PI/jORUiIxX9D+Ei2bfu771P3gyeDbrJfc/cNmJjRqN+D9ogJvkSfT5P1wnrTt5W/s/VM6+kqjC/D9IddDp1yn+P0Ac4kAHkf8/nOH5Sxt8AEAWtYL3si8BQJKIC6NK4wFADFyUTuKWAkCILx36eUoDQAQDpqUR/gNAftYuUamxBED6qbf8QGUFQHR9QKjYGAZA8FDJU3DMBkBsJFL/B4AHQOb32qqfMwhAYstjVjfnCEDcnuwBz5oJQFhyda1mTgpA0kX+WP4BC0BOGYcElrULQMrsD7AtaQxARMCYW8UcDUDAkyEHXdANQDpnqrL0gw5AtjozXow3D0AwDrwJJOsPQNZwotpdTxBAlNpmsCmpEEBSRCuG9QIRQA6u71vBXBFAzBe0MY22EUCKgXgHWRASQEjrPN0kahJABlUBs/DDEkA=\",\"dtype\":\"float64\",\"shape\":[100]},\"top\":{\"__ndarray__\":\"s6auGMn1ZT+zpq4YyfVlPwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAALOmrhjJ9WU/AAAAAAAAAAAAAAAAAAAAALOmrhjJ9WU/k6auGMn1ZT8AAAAAAAAAAJOmrhjJ9XU/AAAAAAAAAAAAAAAAAAAAAJOmrhjJ9WU/0qauGMn1dT+Tpq4YyfV1P7OmrhjJ9YU/Bf2C0lZ4gD+zpq4YyfWFP7OmrhjJ9WU/wdGY9Q83kz9fUNpeO3ObPwX9gtJWeJA/s6auGMn1lT/d0Zj1DzeTP8HRmPUPN6M/9GVPzd4Tqj+Je8Q7grSoP/RlT83eE6o/O3JIGwUosT9zD3sTUZGvP/RlT83eE7o/9GVPzd4Tuj+ht+X2LdDAP+gbdGF3pcY/2lQY73k5zz+nXNOsYYfSP6wBwWKVPtQ/xJV3OmQb2z9Z/6EadlviPyTgAZkQXOU/7sBhF6tc6D9YgiEU4F3uP2El8IH0Me4/XSz8TJet6j/wnIMFB5fnP/EzCBg6Kec/9UZePr7m4z89itzRx6vhP+ED4Kq0IdY/9UZePr7m0z+XmrXKouHOP/nrS/TxncU/Zbwjh2yWxD+hOmXwl9K8P/VGXj6+5rM/mzHpzxpGtT/8kDmqJVWnP2W8I4dslqQ/wdGY9Q83oz8qvCOHbJakP/jRmPUPN5M/wdGY9Q83kz8d/YLSVniQP5OmrhjJ9YU/k6auGMn1hT/Spq4YyfVlP5OmrhjJ9WU/0qauGMn1ZT8AAAAAAAAAAJOmrhjJ9WU/0qauGMn1ZT+Tpq4YyfVlP9KmrhjJ9WU/k6auGMn1dT8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADSpq4YyfVlPwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAk6auGMn1ZT8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAk6auGMn1ZT8=\",\"dtype\":\"float64\",\"shape\":[100]}}},\"id\":\"5c90751c-1793-49bb-8456-c22501b19054\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"dimension\":1,\"plot\":{\"id\":\"3d3a0acb-b2f1-48fd-900f-0c45eae16dbc\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"b943d457-21c1-4b5e-a0dd-9c62ed3adf59\",\"type\":\"BasicTicker\"}},\"id\":\"acced3c4-41de-4dd5-af98-8ba56ffcd365\",\"type\":\"Grid\"},{\"attributes\":{\"plot\":{\"id\":\"3d3a0acb-b2f1-48fd-900f-0c45eae16dbc\",\"subtype\":\"Figure\",\"type\":\"Plot\"}},\"id\":\"481b45bc-fb26-4a0b-be1c-97173aebc981\",\"type\":\"PanTool\"},{\"attributes\":{},\"id\":\"a91d06a2-1a36-440f-a65d-391e38486885\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"plot\":{\"id\":\"3d3a0acb-b2f1-48fd-900f-0c45eae16dbc\",\"subtype\":\"Figure\",\"type\":\"Plot\"}},\"id\":\"5123d3a5-ffa9-4002-882d-71321d0a306e\",\"type\":\"WheelZoomTool\"},{\"attributes\":{\"plot\":{\"id\":\"3d3a0acb-b2f1-48fd-900f-0c45eae16dbc\",\"subtype\":\"Figure\",\"type\":\"Plot\"}},\"id\":\"2721ebe5-714a-44e5-8cb0-8e8a58b4267b\",\"type\":\"ResetTool\"},{\"attributes\":{\"plot\":{\"id\":\"3d3a0acb-b2f1-48fd-900f-0c45eae16dbc\",\"subtype\":\"Figure\",\"type\":\"Plot\"}},\"id\":\"4cce40c5-929e-453e-9fb4-fa5b93726baf\",\"type\":\"SaveTool\"},{\"attributes\":{\"below\":[{\"id\":\"a078390d-84a5-4fb9-bdee-c1b68b2cbf92\",\"type\":\"LinearAxis\"}],\"left\":[{\"id\":\"26861075-037c-412f-8379-1d74034aaa9e\",\"type\":\"LinearAxis\"}],\"renderers\":[{\"id\":\"a078390d-84a5-4fb9-bdee-c1b68b2cbf92\",\"type\":\"LinearAxis\"},{\"id\":\"a6e4ccd6-e0cc-4f10-a1ee-bae9d7f4d375\",\"type\":\"Grid\"},{\"id\":\"26861075-037c-412f-8379-1d74034aaa9e\",\"type\":\"LinearAxis\"},{\"id\":\"acced3c4-41de-4dd5-af98-8ba56ffcd365\",\"type\":\"Grid\"},{\"id\":\"f5b629c3-dbd2-4d6b-bae8-d3cfe3ac261f\",\"type\":\"GlyphRenderer\"}],\"title\":{\"id\":\"fb25d8cf-836d-4ca4-9886-e693de3e88db\",\"type\":\"Title\"},\"tool_events\":{\"id\":\"51e33b7c-c6db-4b8b-b32f-39a417788c79\",\"type\":\"ToolEvents\"},\"toolbar\":{\"id\":\"f2cd8bed-ec2e-4949-972a-2d9ba15218fa\",\"type\":\"Toolbar\"},\"toolbar_location\":\"above\",\"x_range\":{\"id\":\"4fea6e80-d155-4033-bc70-c061803cc245\",\"type\":\"DataRange1d\"},\"y_range\":{\"id\":\"319b6fad-2e3d-42f3-a73d-75e03402ce01\",\"type\":\"DataRange1d\"}},\"id\":\"3d3a0acb-b2f1-48fd-900f-0c45eae16dbc\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{\"plot\":null,\"text\":\"Word positive/negative Affinity Distribution\"},\"id\":\"fb25d8cf-836d-4ca4-9886-e693de3e88db\",\"type\":\"Title\"},{\"attributes\":{},\"id\":\"51e33b7c-c6db-4b8b-b32f-39a417788c79\",\"type\":\"ToolEvents\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"481b45bc-fb26-4a0b-be1c-97173aebc981\",\"type\":\"PanTool\"},{\"id\":\"5123d3a5-ffa9-4002-882d-71321d0a306e\",\"type\":\"WheelZoomTool\"},{\"id\":\"2721ebe5-714a-44e5-8cb0-8e8a58b4267b\",\"type\":\"ResetTool\"},{\"id\":\"4cce40c5-929e-453e-9fb4-fa5b93726baf\",\"type\":\"SaveTool\"}]},\"id\":\"f2cd8bed-ec2e-4949-972a-2d9ba15218fa\",\"type\":\"Toolbar\"},{\"attributes\":{},\"id\":\"74e7aa9b-1532-4b13-8aa6-9f97d4963a86\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"callback\":null},\"id\":\"4fea6e80-d155-4033-bc70-c061803cc245\",\"type\":\"DataRange1d\"},{\"attributes\":{\"formatter\":{\"id\":\"a91d06a2-1a36-440f-a65d-391e38486885\",\"type\":\"BasicTickFormatter\"},\"plot\":{\"id\":\"3d3a0acb-b2f1-48fd-900f-0c45eae16dbc\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"fb53bc5f-385d-4a24-ae47-08d745787b96\",\"type\":\"BasicTicker\"}},\"id\":\"a078390d-84a5-4fb9-bdee-c1b68b2cbf92\",\"type\":\"LinearAxis\"},{\"attributes\":{\"bottom\":{\"value\":0},\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"left\":{\"field\":\"left\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"right\":{\"field\":\"right\"},\"top\":{\"field\":\"top\"}},\"id\":\"a02f0734-eb22-4920-828c-b5717ecf8fae\",\"type\":\"Quad\"},{\"attributes\":{\"callback\":null},\"id\":\"319b6fad-2e3d-42f3-a73d-75e03402ce01\",\"type\":\"DataRange1d\"},{\"attributes\":{},\"id\":\"fb53bc5f-385d-4a24-ae47-08d745787b96\",\"type\":\"BasicTicker\"},{\"attributes\":{\"bottom\":{\"value\":0},\"fill_color\":{\"value\":\"#1f77b4\"},\"left\":{\"field\":\"left\"},\"line_color\":{\"value\":\"#555555\"},\"right\":{\"field\":\"right\"},\"top\":{\"field\":\"top\"}},\"id\":\"880c3728-634d-4b36-b0af-32c19ef9bd2f\",\"type\":\"Quad\"},{\"attributes\":{\"plot\":{\"id\":\"3d3a0acb-b2f1-48fd-900f-0c45eae16dbc\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"fb53bc5f-385d-4a24-ae47-08d745787b96\",\"type\":\"BasicTicker\"}},\"id\":\"a6e4ccd6-e0cc-4f10-a1ee-bae9d7f4d375\",\"type\":\"Grid\"}],\"root_ids\":[\"3d3a0acb-b2f1-48fd-900f-0c45eae16dbc\"]},\"title\":\"Bokeh Application\",\"version\":\"0.12.4\"}};\n",
       "            var render_items = [{\"docid\":\"7e445b23-4bc0-4dfa-b6c8-08386bc4d04c\",\"elementid\":\"cc88d3ef-17a0-4a9d-85fe-8eb4aa4f0334\",\"modelid\":\"3d3a0acb-b2f1-48fd-900f-0c45eae16dbc\"}];\n",
       "            \n",
       "            Bokeh.embed.embed_items(docs_json, render_items);\n",
       "          };\n",
       "          if (document.readyState != \"loading\") fn();\n",
       "          else document.addEventListener(\"DOMContentLoaded\", fn);\n",
       "        })();\n",
       "      },\n",
       "      function(Bokeh) {\n",
       "      }\n",
       "    ];\n",
       "  \n",
       "    function run_inline_js() {\n",
       "      \n",
       "      if ((window.Bokeh !== undefined) || (force === true)) {\n",
       "        for (var i = 0; i < inline_js.length; i++) {\n",
       "          inline_js[i](window.Bokeh);\n",
       "        }if (force === true) {\n",
       "          display_loaded();\n",
       "        }} else if (Date.now() < window._bokeh_timeout) {\n",
       "        setTimeout(run_inline_js, 100);\n",
       "      } else if (!window._bokeh_failed_load) {\n",
       "        console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "        window._bokeh_failed_load = true;\n",
       "      } else if (force !== true) {\n",
       "        var cell = $(document.getElementById(\"cc88d3ef-17a0-4a9d-85fe-8eb4aa4f0334\")).parents('.cell').data().cell;\n",
       "        cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "      }\n",
       "  \n",
       "    }\n",
       "  \n",
       "    if (window._bokeh_is_loading === 0) {\n",
       "      console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "      run_inline_js();\n",
       "    } else {\n",
       "      load_libs(js_urls, function() {\n",
       "        console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "        run_inline_js();\n",
       "      });\n",
       "    }\n",
       "  }(this));\n",
       "</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hist, edges = np.histogram(list(map(lambda x:x[1], pos_neg_ratios.most_common())), density=True, bins=100, normed=True)\n",
    "\n",
    "p = figure(tools='pan,wheel_zoom,reset,save', toolbar_location='above', title=\"Word positive/negative Affinity Distribution\")\n",
    "\n",
    "p.quad(top=hist, bottom=0, left=edges[:-1], right=edges[1:], line_color=\"#555555\")\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that there are a large number of words lying in the middle of the x axis. This shows that there are a large number of words that have little meaning and less words on the sides with meaning.\n",
    "\n",
    "This is good because we can add a cut-off point which will discount the useless words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the data to remove the noise\n",
    "\n",
    "Now we can use what we have learned to increase the accuracy of the network by introducing a fequency cut-off. This will remove the useless words like '.' and ''."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "# Let's tweak the network from before to model these phenomena\n",
    "class SentimentNetwork:\n",
    "    def __init__(self, reviews,labels,min_count = 10,polarity_cutoff = 0.1,hidden_nodes = 10, learning_rate = 0.1):\n",
    "       \n",
    "        np.random.seed(1)\n",
    "    \n",
    "        self.pre_process_data(reviews, polarity_cutoff, min_count)\n",
    "        \n",
    "        self.init_network(len(self.review_vocab),hidden_nodes, 1, learning_rate)\n",
    "        \n",
    "        \n",
    "    def pre_process_data(self,reviews, polarity_cutoff,min_count):\n",
    "        \n",
    "        positive_counts = Counter()\n",
    "        negative_counts = Counter()\n",
    "        total_counts = Counter()\n",
    "\n",
    "        for i in range(len(reviews)):\n",
    "            if(labels[i] == 'POSITIVE'):\n",
    "                for word in reviews[i].split(\" \"):\n",
    "                    positive_counts[word] += 1\n",
    "                    total_counts[word] += 1\n",
    "            else:\n",
    "                for word in reviews[i].split(\" \"):\n",
    "                    negative_counts[word] += 1\n",
    "                    total_counts[word] += 1\n",
    "\n",
    "        pos_neg_ratios = Counter()\n",
    "\n",
    "        for term,cnt in list(total_counts.most_common()):\n",
    "            if(cnt >= 50):\n",
    "                pos_neg_ratio = positive_counts[term] / float(negative_counts[term]+1)\n",
    "                pos_neg_ratios[term] = pos_neg_ratio\n",
    "\n",
    "        for word,ratio in pos_neg_ratios.most_common():\n",
    "            if(ratio > 1):\n",
    "                pos_neg_ratios[word] = np.log(ratio)\n",
    "            else:\n",
    "                pos_neg_ratios[word] = -np.log((1 / (ratio + 0.01)))\n",
    "        \n",
    "        review_vocab = set()\n",
    "        for review in reviews:\n",
    "            for word in review.split(\" \"):\n",
    "                if(total_counts[word] > min_count):\n",
    "                    if(word in pos_neg_ratios.keys()):\n",
    "                        if((pos_neg_ratios[word] >= polarity_cutoff) or (pos_neg_ratios[word] <= -polarity_cutoff)):\n",
    "                            review_vocab.add(word)\n",
    "                    else:\n",
    "                        review_vocab.add(word)\n",
    "        self.review_vocab = list(review_vocab)\n",
    "        \n",
    "        label_vocab = set()\n",
    "        for label in labels:\n",
    "            label_vocab.add(label)\n",
    "        \n",
    "        self.label_vocab = list(label_vocab)\n",
    "        \n",
    "        self.review_vocab_size = len(self.review_vocab)\n",
    "        self.label_vocab_size = len(self.label_vocab)\n",
    "        \n",
    "        self.word2index = {}\n",
    "        for i, word in enumerate(self.review_vocab):\n",
    "            self.word2index[word] = i\n",
    "        \n",
    "        self.label2index = {}\n",
    "        for i, label in enumerate(self.label_vocab):\n",
    "            self.label2index[label] = i\n",
    "         \n",
    "        \n",
    "    def init_network(self, input_nodes, hidden_nodes, output_nodes, learning_rate):\n",
    "        # Set number of nodes in input, hidden and output layers.\n",
    "        self.input_nodes = input_nodes\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.output_nodes = output_nodes\n",
    "\n",
    "        # Initialize weights\n",
    "        self.weights_0_1 = np.zeros((self.input_nodes,self.hidden_nodes))\n",
    "    \n",
    "        self.weights_1_2 = np.random.normal(0.0, self.output_nodes**-0.5, \n",
    "                                                (self.hidden_nodes, self.output_nodes))\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.layer_0 = np.zeros((1,input_nodes))\n",
    "        self.layer_1 = np.zeros((1,hidden_nodes))\n",
    "        \n",
    "    def sigmoid(self,x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    \n",
    "    def sigmoid_output_2_derivative(self,output):\n",
    "        return output * (1 - output)\n",
    "    \n",
    "    def update_input_layer(self,review):\n",
    "\n",
    "        # clear out previous state, reset the layer to be all 0s\n",
    "        self.layer_0 *= 0\n",
    "        for word in review.split(\" \"):\n",
    "            self.layer_0[0][self.word2index[word]] = 1\n",
    "\n",
    "    def get_target_for_label(self,label):\n",
    "        if(label == 'POSITIVE'):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def train(self, training_reviews_raw, training_labels):\n",
    "        \n",
    "        training_reviews = list()\n",
    "        for review in training_reviews_raw:\n",
    "            indices = set()\n",
    "            for word in review.split(\" \"):\n",
    "                if(word in self.word2index.keys()):\n",
    "                    indices.add(self.word2index[word])\n",
    "            training_reviews.append(list(indices))\n",
    "        \n",
    "        assert(len(training_reviews) == len(training_labels))\n",
    "        \n",
    "        correct_so_far = 0\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        for i in range(len(training_reviews)):\n",
    "            \n",
    "            review = training_reviews[i]\n",
    "            label = training_labels[i]\n",
    "            \n",
    "            #### Implement the forward pass here ####\n",
    "            ### Forward pass ###\n",
    "\n",
    "            # Input Layer\n",
    "\n",
    "            # Hidden layer\n",
    "#             layer_1 = self.layer_0.dot(self.weights_0_1)\n",
    "            self.layer_1 *= 0\n",
    "            for index in review:\n",
    "                self.layer_1 += self.weights_0_1[index]\n",
    "            \n",
    "            # Output layer\n",
    "            layer_2 = self.sigmoid(self.layer_1.dot(self.weights_1_2))\n",
    "\n",
    "            #### Implement the backward pass here ####\n",
    "            ### Backward pass ###\n",
    "\n",
    "            # Output error\n",
    "            layer_2_error = layer_2 - self.get_target_for_label(label) # Output layer error is the difference between desired target and actual output.\n",
    "            layer_2_delta = layer_2_error * self.sigmoid_output_2_derivative(layer_2)\n",
    "\n",
    "            # Backpropagated error\n",
    "            layer_1_error = layer_2_delta.dot(self.weights_1_2.T) # errors propagated to the hidden layer\n",
    "            layer_1_delta = layer_1_error # hidden layer gradients - no nonlinearity so it's the same as the error\n",
    "\n",
    "            # Update the weights\n",
    "            self.weights_1_2 -= self.layer_1.T.dot(layer_2_delta) * self.learning_rate # update hidden-to-output weights with gradient descent step\n",
    "            \n",
    "            for index in review:\n",
    "                self.weights_0_1[index] -= layer_1_delta[0] * self.learning_rate # update input-to-hidden weights with gradient descent step\n",
    "\n",
    "            if(layer_2 >= 0.5 and label == 'POSITIVE'):\n",
    "                correct_so_far += 1\n",
    "            if(layer_2 < 0.5 and label == 'NEGATIVE'):\n",
    "                correct_so_far += 1\n",
    "            \n",
    "            reviews_per_second = i / float(time.time() - start)\n",
    "            \n",
    "            sys.stdout.write(\"\\rProgress:\" + str(100 * i/float(len(training_reviews)))[:4] + \"% Speed(reviews/sec):\" + str(reviews_per_second)[0:5] + \" #Correct:\" + str(correct_so_far) + \" #Trained:\" + str(i+1) + \" Training Accuracy:\" + str(correct_so_far * 100 / float(i+1))[:4] + \"%\")\n",
    "        \n",
    "    \n",
    "    def test(self, testing_reviews, testing_labels):\n",
    "        \n",
    "        correct = 0\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        for i in range(len(testing_reviews)):\n",
    "            pred = self.run(testing_reviews[i])\n",
    "            if(pred == testing_labels[i]):\n",
    "                correct += 1\n",
    "            \n",
    "            reviews_per_second = i / float(time.time() - start)\n",
    "            \n",
    "            sys.stdout.write(\"\\rProgress:\" + str(100 * i/float(len(testing_reviews)))[:4] \\\n",
    "                             + \"% Speed(reviews/sec):\" + str(reviews_per_second)[0:5] \\\n",
    "                            + \"% #Correct:\" + str(correct) + \" #Tested:\" + str(i+1) + \" Testing Accuracy:\" + str(correct * 100 / float(i+1))[:4] + \"%\")\n",
    "    \n",
    "    def run(self, review):\n",
    "        \n",
    "        # Input Layer\n",
    "\n",
    "\n",
    "        # Hidden layer\n",
    "        self.layer_1 *= 0\n",
    "        unique_indices = set()\n",
    "        for word in review.lower().split(\" \"):\n",
    "            if word in self.word2index.keys():\n",
    "                unique_indices.add(self.word2index[word])\n",
    "        for index in unique_indices:\n",
    "            self.layer_1 += self.weights_0_1[index]\n",
    "        \n",
    "        # Output layer\n",
    "        layer_2 = self.sigmoid(self.layer_1.dot(self.weights_1_2))\n",
    "        \n",
    "        if(layer_2[0] >= 0.5):\n",
    "            return \"POSITIVE\"\n",
    "        else:\n",
    "            return \"NEGATIVE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "min_count_cutoff: In order to be included it must be more frequent than this value\n",
    "polarity_cutoff: Words must be left or right of the histogram by this much to be included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp = SentimentNetwork(reviews[:-1000],labels[:-1000],min_count=20,polarity_cutoff=0.05,learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress:99.9% Speed(reviews/sec):2160. #Correct:20461 #Trained:24000 Training Accuracy:85.2%"
     ]
    }
   ],
   "source": [
    "mlp.train(reviews[:-1000],labels[:-1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress:99.9% Speed(reviews/sec):2551.% #Correct:859 #Tested:1000 Testing Accuracy:85.9%"
     ]
    }
   ],
   "source": [
    "mlp.test(reviews[-1000:],labels[-1000:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reducing the useless data has clearly had a positive impact on the overall accuracy of the network during testing. It has also had a small increase in the speed because the dictionary is smaller. \n",
    "\n",
    "Increasing the polarity cutoff will speed up the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp = SentimentNetwork(reviews[:-1000],labels[:-1000],min_count=20,polarity_cutoff=0.8,learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress:99.9% Speed(reviews/sec):7229. #Correct:20552 #Trained:24000 Training Accuracy:85.6%"
     ]
    }
   ],
   "source": [
    "mlp.train(reviews[:-1000],labels[:-1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress:99.9% Speed(reviews/sec):4765.% #Correct:822 #Tested:1000 Testing Accuracy:82.2%"
     ]
    }
   ],
   "source": [
    "mlp.test(reviews[-1000:],labels[-1000:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we got an increase in speed from 2236 w/s to 6251 w/s however 3% of the accuracy was traded off as a result. There will almost always be a trade-off between speed and accuracy excluding noise reduction which normally helps both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis - Whats going on with the weights?\n",
    "\n",
    "Here we will visualise what is realling going on under the hood of the network and how the weights work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp_full = SentimentNetwork(reviews[:-1000], labels[:-1000], min_count=0, polarity_cutoff=0, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress:99.9% Speed(reviews/sec):1804. #Correct:20335 #Trained:24000 Training Accuracy:84.7%"
     ]
    }
   ],
   "source": [
    "mlp_full.train(reviews[:-1000], labels[:-1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_most_similar_words(focus = \"horrible\"):\n",
    "    most_similar = Counter()\n",
    "\n",
    "    for word in mlp_full.word2index.keys():\n",
    "        most_similar[word] = np.dot(mlp_full.weights_0_1[mlp_full.word2index[word]],mlp_full.weights_0_1[mlp_full.word2index[focus]])\n",
    "    \n",
    "    return most_similar.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('excellent', 0.13672950757352473),\n",
       " ('perfect', 0.12548286087225946),\n",
       " ('amazing', 0.091827633925999713),\n",
       " ('today', 0.090223662694414231),\n",
       " ('wonderful', 0.089355976962214617),\n",
       " ('fun', 0.087504466674206888),\n",
       " ('great', 0.087141758882292031),\n",
       " ('best', 0.085810885617880639),\n",
       " ('liked', 0.07769762912384344),\n",
       " ('definitely', 0.076628781406966023),\n",
       " ('brilliant', 0.073423858769279038),\n",
       " ('loved', 0.073285428928122148),\n",
       " ('favorite', 0.072781136036160751),\n",
       " ('superb', 0.071736207178505068),\n",
       " ('fantastic', 0.070922191916266197),\n",
       " ('job', 0.069160617207634056),\n",
       " ('incredible', 0.06642407795261443),\n",
       " ('enjoyable', 0.065632560502888793),\n",
       " ('rare', 0.064819212662615075),\n",
       " ('highly', 0.063889453350970515),\n",
       " ('enjoyed', 0.062127546101812953),\n",
       " ('wonderfully', 0.062055178604090155),\n",
       " ('perfectly', 0.061093208811887401),\n",
       " ('fascinating', 0.060663547937493886),\n",
       " ('bit', 0.059655427045653034),\n",
       " ('gem', 0.059510859296156786),\n",
       " ('outstanding', 0.058860808147083013),\n",
       " ('beautiful', 0.058613934703162042),\n",
       " ('surprised', 0.058273314482562975),\n",
       " ('worth', 0.057657484236471199),\n",
       " ('especially', 0.057422020781760785),\n",
       " ('refreshing', 0.057310532092265769),\n",
       " ('entertaining', 0.056612033835629218),\n",
       " ('hilarious', 0.056168541032286662),\n",
       " ('masterpiece', 0.054993988649431544),\n",
       " ('simple', 0.054484083134924047),\n",
       " ('subtle', 0.054368883033508647),\n",
       " ('funniest', 0.053457164871302691),\n",
       " ('solid', 0.052903564743620658),\n",
       " ('awesome', 0.052489194202770428),\n",
       " ('always', 0.052260328525345269),\n",
       " ('noir', 0.051530194726406908),\n",
       " ('guys', 0.051109413645642685),\n",
       " ('sweet', 0.050818930317525997),\n",
       " ('unique', 0.050670162263589183),\n",
       " ('very', 0.05013299494852845),\n",
       " ('heart', 0.04994805849824361),\n",
       " ('moving', 0.049424601164379134),\n",
       " ('atmosphere', 0.048842500895912848),\n",
       " ('strong', 0.048570880631759197)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_most_similar_words('excellent')[0:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because these words are supposed to give a similar output the network sees them as similar and therefore have similar weights. We can visualise these clusters buy plotting them on a graph using T-SNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.colors as colors\n",
    "\n",
    "words_to_visualize = list()\n",
    "for word, ratio in pos_neg_ratios.most_common(500):\n",
    "    if(word in mlp_full.word2index.keys()):\n",
    "        words_to_visualize.append(word)\n",
    "    \n",
    "for word, ratio in list(reversed(pos_neg_ratios.most_common()))[0:500]:\n",
    "    if(word in mlp_full.word2index.keys()):\n",
    "        words_to_visualize.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos = 0\n",
    "neg = 0\n",
    "\n",
    "colors_list = list()\n",
    "vectors_list = list()\n",
    "for word in words_to_visualize:\n",
    "    if word in pos_neg_ratios.keys():\n",
    "        vectors_list.append(mlp_full.weights_0_1[mlp_full.word2index[word]])\n",
    "        if(pos_neg_ratios[word] > 0):\n",
    "            pos+=1\n",
    "            colors_list.append(\"#00ff00\")\n",
    "        else:\n",
    "            neg+=1\n",
    "            colors_list.append(\"#000000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "words_top_ted_tsne = tsne.fit_transform(vectors_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/max/anaconda/lib/python3.6/site-packages/bokeh/util/deprecation.py:34: BokehDeprecationWarning: \n",
      "Supplying a user-defined data source AND iterable values to glyph methods is deprecated.\n",
      "\n",
      "See https://github.com/bokeh/bokeh/issues/2056 for more information.\n",
      "\n",
      "  warn(message)\n",
      "/Users/max/anaconda/lib/python3.6/site-packages/bokeh/util/deprecation.py:34: BokehDeprecationWarning: \n",
      "Supplying a user-defined data source AND iterable values to glyph methods is deprecated.\n",
      "\n",
      "See https://github.com/bokeh/bokeh/issues/2056 for more information.\n",
      "\n",
      "  warn(message)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <div class=\"bk-plotdiv\" id=\"431dd0c9-b460-488f-b329-bf72552c6a32\"></div>\n",
       "    </div>\n",
       "<script type=\"text/javascript\">\n",
       "  \n",
       "  (function(global) {\n",
       "    function now() {\n",
       "      return new Date();\n",
       "    }\n",
       "  \n",
       "    var force = false;\n",
       "  \n",
       "    if (typeof (window._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "      window._bokeh_onload_callbacks = [];\n",
       "      window._bokeh_is_loading = undefined;\n",
       "    }\n",
       "  \n",
       "  \n",
       "    \n",
       "    if (typeof (window._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "      window._bokeh_timeout = Date.now() + 0;\n",
       "      window._bokeh_failed_load = false;\n",
       "    }\n",
       "  \n",
       "    var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "       \"<div style='background-color: #fdd'>\\n\"+\n",
       "       \"<p>\\n\"+\n",
       "       \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "       \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "       \"</p>\\n\"+\n",
       "       \"<ul>\\n\"+\n",
       "       \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "       \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "       \"</ul>\\n\"+\n",
       "       \"<code>\\n\"+\n",
       "       \"from bokeh.resources import INLINE\\n\"+\n",
       "       \"output_notebook(resources=INLINE)\\n\"+\n",
       "       \"</code>\\n\"+\n",
       "       \"</div>\"}};\n",
       "  \n",
       "    function display_loaded() {\n",
       "      if (window.Bokeh !== undefined) {\n",
       "        document.getElementById(\"431dd0c9-b460-488f-b329-bf72552c6a32\").textContent = \"BokehJS successfully loaded.\";\n",
       "      } else if (Date.now() < window._bokeh_timeout) {\n",
       "        setTimeout(display_loaded, 100)\n",
       "      }\n",
       "    }\n",
       "  \n",
       "    function run_callbacks() {\n",
       "      window._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "      delete window._bokeh_onload_callbacks\n",
       "      console.info(\"Bokeh: all callbacks have finished\");\n",
       "    }\n",
       "  \n",
       "    function load_libs(js_urls, callback) {\n",
       "      window._bokeh_onload_callbacks.push(callback);\n",
       "      if (window._bokeh_is_loading > 0) {\n",
       "        console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "        return null;\n",
       "      }\n",
       "      if (js_urls == null || js_urls.length === 0) {\n",
       "        run_callbacks();\n",
       "        return null;\n",
       "      }\n",
       "      console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "      window._bokeh_is_loading = js_urls.length;\n",
       "      for (var i = 0; i < js_urls.length; i++) {\n",
       "        var url = js_urls[i];\n",
       "        var s = document.createElement('script');\n",
       "        s.src = url;\n",
       "        s.async = false;\n",
       "        s.onreadystatechange = s.onload = function() {\n",
       "          window._bokeh_is_loading--;\n",
       "          if (window._bokeh_is_loading === 0) {\n",
       "            console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "            run_callbacks()\n",
       "          }\n",
       "        };\n",
       "        s.onerror = function() {\n",
       "          console.warn(\"failed to load library \" + url);\n",
       "        };\n",
       "        console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      }\n",
       "    };var element = document.getElementById(\"431dd0c9-b460-488f-b329-bf72552c6a32\");\n",
       "    if (element == null) {\n",
       "      console.log(\"Bokeh: ERROR: autoload.js configured with elementid '431dd0c9-b460-488f-b329-bf72552c6a32' but no matching script tag was found. \")\n",
       "      return false;\n",
       "    }\n",
       "  \n",
       "    var js_urls = [];\n",
       "  \n",
       "    var inline_js = [\n",
       "      function(Bokeh) {\n",
       "        (function() {\n",
       "          var fn = function() {\n",
       "            var docs_json = {\"a71d9186-081f-4735-b10e-a42440bed059\":{\"roots\":{\"references\":[{\"attributes\":{\"formatter\":{\"id\":\"1250c23e-c515-4298-a204-c3dee3664f16\",\"type\":\"BasicTickFormatter\"},\"plot\":{\"id\":\"fd87fb36-4a56-477c-b8b7-107ab7e5f56b\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"296ea7e0-d9b9-4b34-9c2d-536801e12ecd\",\"type\":\"BasicTicker\"}},\"id\":\"a0fcd5ec-151a-4c14-b506-dd8d52455051\",\"type\":\"LinearAxis\"},{\"attributes\":{},\"id\":\"1250c23e-c515-4298-a204-c3dee3664f16\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{},\"id\":\"296ea7e0-d9b9-4b34-9c2d-536801e12ecd\",\"type\":\"BasicTicker\"},{\"attributes\":{\"below\":[{\"id\":\"1a6cab84-4756-4c66-b429-b3c59e15b790\",\"type\":\"LinearAxis\"}],\"left\":[{\"id\":\"a0fcd5ec-151a-4c14-b506-dd8d52455051\",\"type\":\"LinearAxis\"}],\"renderers\":[{\"id\":\"1a6cab84-4756-4c66-b429-b3c59e15b790\",\"type\":\"LinearAxis\"},{\"id\":\"30a42cc3-61c2-42f3-bf39-6df2c79959e2\",\"type\":\"Grid\"},{\"id\":\"a0fcd5ec-151a-4c14-b506-dd8d52455051\",\"type\":\"LinearAxis\"},{\"id\":\"65c152bb-6a92-4055-b104-de140d811ed5\",\"type\":\"Grid\"},{\"id\":\"05523265-9e10-478a-be5c-fced408d95c6\",\"type\":\"GlyphRenderer\"},{\"id\":\"b6aec15f-6b34-4f8e-970b-e221ba5855da\",\"type\":\"LabelSet\"}],\"title\":{\"id\":\"b1a62e8e-500e-4943-a845-beddeea68ba3\",\"type\":\"Title\"},\"tool_events\":{\"id\":\"6b30ed16-04bf-43b5-87db-37cf997e92c9\",\"type\":\"ToolEvents\"},\"toolbar\":{\"id\":\"dd7151fb-d8d7-484c-a1bf-cfca93c3a34f\",\"type\":\"Toolbar\"},\"toolbar_location\":\"above\",\"x_range\":{\"id\":\"10f4d84c-11f4-44de-a545-92679213fa89\",\"type\":\"DataRange1d\"},\"y_range\":{\"id\":\"f12a1ff3-e572-4561-800e-1332f80edf01\",\"type\":\"DataRange1d\"}},\"id\":\"fd87fb36-4a56-477c-b8b7-107ab7e5f56b\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{\"callback\":null,\"column_names\":[\"x1\",\"x2\",\"names\",\"fill_color\",\"line_color\"],\"data\":{\"fill_color\":[\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\"],\"line_color\":[\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#00ff00\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\",\"#000000\"],\"names\":[\"edie\",\"paulie\",\"felix\",\"polanski\",\"matthau\",\"victoria\",\"mildred\",\"gandhi\",\"flawless\",\"superbly\",\"perfection\",\"astaire\",\"captures\",\"voight\",\"wonderfully\",\"powell\",\"brosnan\",\"lily\",\"bakshi\",\"lincoln\",\"refreshing\",\"breathtaking\",\"bourne\",\"lemmon\",\"delightful\",\"flynn\",\"andrews\",\"homer\",\"beautifully\",\"soccer\",\"elvira\",\"underrated\",\"gripping\",\"superb\",\"delight\",\"welles\",\"sadness\",\"sinatra\",\"touching\",\"timeless\",\"macy\",\"unforgettable\",\"favorites\",\"stewart\",\"sullivan\",\"extraordinary\",\"hartley\",\"brilliantly\",\"friendship\",\"wonderful\",\"palma\",\"magnificent\",\"finest\",\"jackie\",\"ritter\",\"tremendous\",\"freedom\",\"fantastic\",\"terrific\",\"noir\",\"sidney\",\"outstanding\",\"pleasantly\",\"mann\",\"nancy\",\"marie\",\"marvelous\",\"excellent\",\"ruth\",\"stanwyck\",\"widmark\",\"splendid\",\"chan\",\"exceptional\",\"tender\",\"gentle\",\"poignant\",\"gem\",\"amazing\",\"chilling\",\"fisher\",\"davies\",\"captivating\",\"darker\",\"april\",\"kelly\",\"blake\",\"overlooked\",\"ralph\",\"bette\",\"hoffman\",\"cole\",\"shines\",\"powerful\",\"notch\",\"remarkable\",\"pitt\",\"winters\",\"vivid\",\"gritty\",\"giallo\",\"portrait\",\"innocence\",\"psychiatrist\",\"favorite\",\"ensemble\",\"stunning\",\"burns\",\"garbo\",\"barbara\",\"philip\",\"panic\",\"holly\",\"carol\",\"perfect\",\"appreciated\",\"favourite\",\"journey\",\"rural\",\"bond\",\"builds\",\"brilliant\",\"brooklyn\",\"von\",\"recommended\",\"unfolds\",\"daniel\",\"perfectly\",\"crafted\",\"prince\",\"troubled\",\"consequences\",\"haunting\",\"cinderella\",\"alexander\",\"emotions\",\"boxing\",\"subtle\",\"curtis\",\"rare\",\"loved\",\"daughters\",\"courage\",\"dentist\",\"highly\",\"nominated\",\"tony\",\"draws\",\"everyday\",\"contrast\",\"cried\",\"fabulous\",\"ned\",\"fay\",\"emma\",\"sensitive\",\"smooth\",\"dramas\",\"today\",\"helps\",\"inspiring\",\"jimmy\",\"awesome\",\"unique\",\"tragic\",\"intense\",\"stellar\",\"rival\",\"provides\",\"depression\",\"shy\",\"carrie\",\"blend\",\"hank\",\"diana\",\"adorable\",\"unexpected\",\"achievement\",\"bettie\",\"happiness\",\"glorious\",\"davis\",\"terrifying\",\"beauty\",\"ideal\",\"fears\",\"hong\",\"seasons\",\"fascinating\",\"carries\",\"satisfying\",\"definite\",\"touched\",\"greatest\",\"creates\",\"aunt\",\"walter\",\"spectacular\",\"portrayal\",\"ann\",\"enterprise\",\"musicals\",\"deeply\",\"incredible\",\"mature\",\"triumph\",\"margaret\",\"navy\",\"harry\",\"lucas\",\"sweet\",\"joey\",\"oscar\",\"balance\",\"warm\",\"ages\",\"guilt\",\"glover\",\"carrey\",\"learns\",\"unusual\",\"sons\",\"complex\",\"essence\",\"brazil\",\"widow\",\"solid\",\"beautiful\",\"holmes\",\"awe\",\"vhs\",\"eerie\",\"lonely\",\"grim\",\"sport\",\"debut\",\"destiny\",\"thrillers\",\"tears\",\"rose\",\"feelings\",\"ginger\",\"winning\",\"stanley\",\"cox\",\"paris\",\"heart\",\"hooked\",\"comfortable\",\"mgm\",\"masterpiece\",\"themes\",\"danny\",\"anime\",\"perry\",\"joy\",\"lovable\",\"mysteries\",\"hal\",\"louis\",\"charming\",\"urban\",\"allows\",\"impact\",\"italy\",\"gradually\",\"lifestyle\",\"spy\",\"treat\",\"subsequent\",\"kennedy\",\"loving\",\"surprising\",\"quiet\",\"winter\",\"reveals\",\"raw\",\"funniest\",\"pleased\",\"norman\",\"thief\",\"season\",\"secrets\",\"colorful\",\"highest\",\"compelling\",\"danes\",\"castle\",\"kudos\",\"great\",\"baseball\",\"subtitles\",\"bleak\",\"winner\",\"tragedy\",\"todd\",\"nicely\",\"arthur\",\"essential\",\"gorgeous\",\"fonda\",\"eastwood\",\"focuses\",\"enjoyed\",\"natural\",\"intensity\",\"witty\",\"rob\",\"worlds\",\"health\",\"magical\",\"deeper\",\"lucy\",\"moving\",\"lovely\",\"purple\",\"memorable\",\"sings\",\"craig\",\"modesty\",\"relate\",\"episodes\",\"strong\",\"smith\",\"tear\",\"apartment\",\"princess\",\"disagree\",\"kung\",\"adventure\",\"columbo\",\"jake\",\"adds\",\"hart\",\"strength\",\"realizes\",\"dave\",\"childhood\",\"forbidden\",\"tight\",\"surreal\",\"manager\",\"dancer\",\"studios\",\"con\",\"miike\",\"realistic\",\"explicit\",\"kurt\",\"traditional\",\"deals\",\"holds\",\"carl\",\"touches\",\"gene\",\"albert\",\"abc\",\"cry\",\"sides\",\"develops\",\"eyre\",\"dances\",\"oscars\",\"legendary\",\"hearted\",\"importance\",\"portraying\",\"impressed\",\"waters\",\"empire\",\"edge\",\"jean\",\"environment\",\"sentimental\",\"captured\",\"styles\",\"daring\",\"frank\",\"tense\",\"backgrounds\",\"matches\",\"gothic\",\"sharp\",\"achieved\",\"court\",\"steals\",\"rules\",\"colors\",\"reunion\",\"covers\",\"tale\",\"rain\",\"denzel\",\"stays\",\"blob\",\"maria\",\"conventional\",\"fresh\",\"midnight\",\"landscape\",\"animated\",\"titanic\",\"sunday\",\"spring\",\"cagney\",\"enjoyable\",\"immensely\",\"sir\",\"nevertheless\",\"driven\",\"performances\",\"memories\",\"nowadays\",\"simple\",\"golden\",\"leslie\",\"lovers\",\"relationship\",\"supporting\",\"che\",\"packed\",\"trek\",\"provoking\",\"strikes\",\"depiction\",\"emotional\",\"secretary\",\"influenced\",\"florida\",\"germany\",\"brings\",\"lewis\",\"elderly\",\"owner\",\"streets\",\"henry\",\"portrays\",\"bears\",\"china\",\"anger\",\"society\",\"available\",\"best\",\"bugs\",\"magic\",\"delivers\",\"verhoeven\",\"jim\",\"donald\",\"endearing\",\"relationships\",\"greatly\",\"charlie\",\"brad\",\"simon\",\"effectively\",\"march\",\"atmosphere\",\"influence\",\"genius\",\"emotionally\",\"ken\",\"identity\",\"sophisticated\",\"dan\",\"andrew\",\"india\",\"roy\",\"surprisingly\",\"sky\",\"romantic\",\"match\",\"meets\",\"cowboy\",\"wave\",\"bitter\",\"patient\",\"stylish\",\"britain\",\"affected\",\"beatty\",\"love\",\"paul\",\"andy\",\"performance\",\"patrick\",\"unlike\",\"brooks\",\"refuses\",\"award\",\"complaint\",\"ride\",\"dawson\",\"luke\",\"wells\",\"france\",\"sports\",\"handsome\",\"directs\",\"rebel\",\"boll\",\"uwe\",\"seagal\",\"unwatchable\",\"stinker\",\"mst\",\"incoherent\",\"unfunny\",\"waste\",\"blah\",\"horrid\",\"pointless\",\"atrocious\",\"redeeming\",\"prom\",\"drivel\",\"lousy\",\"worst\",\"laughable\",\"awful\",\"poorly\",\"wasting\",\"remotely\",\"existent\",\"boredom\",\"miserably\",\"sucks\",\"uninspired\",\"lame\",\"insult\",\"godzilla\",\"uninteresting\",\"gadget\",\"appalling\",\"unconvincing\",\"unintentional\",\"horrible\",\"amateurish\",\"pathetic\",\"idiotic\",\"stupidity\",\"cardboard\",\"wasted\",\"crap\",\"insulting\",\"tedious\",\"dreadful\",\"dire\",\"badly\",\"suck\",\"worse\",\"terrible\",\"embarrassing\",\"mess\",\"garbage\",\"pile\",\"stupid\",\"ashamed\",\"vampires\",\"worthless\",\"dull\",\"inept\",\"avoid\",\"wooden\",\"forgettable\",\"fulci\",\"crappy\",\"bat\",\"unbelievably\",\"whatsoever\",\"excuse\",\"rubbish\",\"ridiculous\",\"junk\",\"flop\",\"boring\",\"turkey\",\"shark\",\"topless\",\"ridiculously\",\"useless\",\"seed\",\"ripped\",\"embarrassed\",\"rambo\",\"costs\",\"hideous\",\"horrendous\",\"bother\",\"dumb\",\"disjointed\",\"plastic\",\"horribly\",\"fest\",\"ludicrous\",\"unintentionally\",\"obnoxious\",\"mildly\",\"bland\",\"mummy\",\"annoying\",\"amateur\",\"bad\",\"dinosaurs\",\"unless\",\"fails\",\"mediocre\",\"awake\",\"clichd\",\"clich\",\"meaningless\",\"disappointment\",\"zombies\",\"asleep\",\"miscast\",\"irritating\",\"utter\",\"disappointing\",\"screaming\",\"supposed\",\"kidding\",\"poor\",\"apes\",\"unbelievable\",\"fake\",\"dude\",\"dracula\",\"joke\",\"clumsy\",\"random\",\"cheap\",\"idiots\",\"devoid\",\"trite\",\"wannabe\",\"unbearable\",\"alright\",\"pretentious\",\"scooby\",\"sucked\",\"senseless\",\"bo\",\"bin\",\"coherent\",\"idiot\",\"toilet\",\"doo\",\"werewolf\",\"cabin\",\"generous\",\"offensive\",\"monkey\",\"painfully\",\"renting\",\"lazy\",\"disgusting\",\"blame\",\"walked\",\"seconds\",\"generic\",\"cheese\",\"sloppy\",\"huh\",\"retarded\",\"trash\",\"shelf\",\"ugly\",\"oh\",\"slightest\",\"explanation\",\"failed\",\"cringe\",\"blatant\",\"clue\",\"bored\",\"cgi\",\"sat\",\"paid\",\"warn\",\"painful\",\"nowhere\",\"bore\",\"absurd\",\"flies\",\"paint\",\"porn\",\"paper\",\"predictable\",\"pseudo\",\"repetitive\",\"outer\",\"brain\",\"sorry\",\"vampire\",\"motivation\",\"unrealistic\",\"wrestling\",\"overrated\",\"aliens\",\"halfway\",\"save\",\"santa\",\"security\",\"contrived\",\"lacks\",\"whale\",\"gore\",\"bunch\",\"hype\",\"flat\",\"noise\",\"below\",\"plain\",\"spending\",\"bothered\",\"annoyed\",\"sounded\",\"honestly\",\"minutes\",\"wreck\",\"lesbian\",\"chick\",\"dollar\",\"f\",\"secondly\",\"wanna\",\"rat\",\"errors\",\"shallow\",\"synopsis\",\"breasts\",\"gray\",\"yeah\",\"nonsense\",\"unnecessary\",\"swear\",\"grave\",\"ruined\",\"somebody\",\"elvis\",\"mindless\",\"terribly\",\"continuity\",\"hoping\",\"ha\",\"nudity\",\"endless\",\"decent\",\"torture\",\"rented\",\"disaster\",\"downright\",\"ok\",\"fat\",\"unpleasant\",\"figured\",\"rip\",\"throwing\",\"attempt\",\"weak\",\"slap\",\"jesus\",\"christian\",\"barely\",\"apparently\",\"implausible\",\"nothing\",\"clichs\",\"credibility\",\"bible\",\"explained\",\"presumably\",\"celluloid\",\"couldn\",\"money\",\"snake\",\"hollow\",\"load\",\"sake\",\"total\",\"priest\",\"supposedly\",\"consists\",\"zombie\",\"bomb\",\"ape\",\"bottom\",\"christ\",\"unfortunately\",\"bullets\",\"grade\",\"drags\",\"freak\",\"wolf\",\"fx\",\"offended\",\"script\",\"raped\",\"producers\",\"okay\",\"confusing\",\"stomach\",\"monster\",\"seriously\",\"alas\",\"promising\",\"knife\",\"substance\",\"premise\",\"threw\",\"k\",\"dear\",\"z\",\"write\",\"rental\",\"warned\",\"zero\",\"semi\",\"guess\",\"scientist\",\"logic\",\"vague\",\"slasher\",\"throw\",\"accents\",\"alien\",\"silly\",\"clown\",\"skip\",\"instead\",\"blank\",\"throat\",\"lab\",\"par\",\"gag\",\"execution\",\"nose\",\"hated\",\"effort\",\"shoot\",\"fill\",\"gratuitous\",\"burn\",\"none\",\"cameras\",\"assume\",\"stick\",\"reasonable\",\"failure\",\"pie\",\"rent\",\"dubbing\",\"weren\",\"truck\",\"stock\",\"thin\",\"daddy\",\"holy\",\"exercise\",\"pg\",\"arm\",\"tried\",\"suppose\",\"advice\",\"gonna\",\"disbelief\",\"derek\",\"mean\",\"merit\",\"looked\",\"channel\",\"gross\",\"stereotypical\",\"hoped\",\"lacking\",\"spent\",\"stiff\",\"overdone\",\"low\",\"romero\",\"hour\",\"blair\",\"saved\",\"damage\",\"reason\",\"intentions\",\"sentence\",\"hardcore\",\"makeup\",\"lack\",\"makers\",\"empty\",\"holes\",\"wouldn\",\"proof\",\"demon\",\"toys\",\"doll\",\"utterly\",\"originality\",\"bush\",\"saying\",\"cover\",\"meat\",\"forest\",\"deserve\",\"sum\",\"bucks\",\"hills\",\"watchable\",\"lacked\",\"handed\",\"mistake\",\"please\",\"whoever\",\"sadistic\",\"monsters\",\"screenwriter\",\"neither\",\"nuclear\",\"sequels\",\"flesh\",\"lying\",\"creature\",\"annie\",\"propaganda\",\"leonard\",\"thats\",\"racist\",\"convince\",\"asian\",\"why\",\"rex\",\"satan\",\"remake\",\"fail\",\"ah\",\"loser\",\"favor\",\"except\",\"flick\",\"freddy\",\"relies\",\"spare\",\"dialog\",\"lou\",\"dragged\",\"guy\",\"problem\",\"melting\",\"flash\",\"im\",\"least\",\"mouth\",\"sole\",\"hell\",\"jerk\",\"drink\",\"intent\",\"shower\",\"fifteen\",\"wasn\",\"thugs\",\"corpse\",\"virus\",\"idea\",\"budget\",\"minimal\",\"reasonably\",\"naked\",\"rick\",\"category\",\"cheesy\",\"judging\",\"half\",\"pregnant\",\"no\",\"millions\",\"stereotypes\",\"juvenile\",\"weekend\",\"convoluted\",\"laurel\",\"killings\",\"sequel\",\"hire\",\"somewhere\",\"frankly\",\"paying\",\"someone\",\"cant\",\"cash\",\"research\",\"dimensional\",\"walk\",\"editing\",\"conceived\",\"scare\",\"positive\",\"anything\"],\"x1\":{\"__ndarray__\":\"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\",\"dtype\":\"float64\",\"shape\":[1000]},\"x2\":{\"__ndarray__\":\"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\",\"dtype\":\"float64\",\"shape\":[1000]}}},\"id\":\"3ddbb301-7cb2-4edd-aed7-b77feb56f67a\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"callback\":null},\"id\":\"f12a1ff3-e572-4561-800e-1332f80edf01\",\"type\":\"DataRange1d\"},{\"attributes\":{},\"id\":\"9fc50889-5464-4d39-995d-eb976161d778\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"plot\":null,\"text\":\"vector T-SNE for most polarized words\"},\"id\":\"b1a62e8e-500e-4943-a845-beddeea68ba3\",\"type\":\"Title\"},{\"attributes\":{},\"id\":\"829f4a6d-049b-4010-94b7-d587a0c51dd0\",\"type\":\"BasicTicker\"},{\"attributes\":{\"plot\":{\"id\":\"fd87fb36-4a56-477c-b8b7-107ab7e5f56b\",\"subtype\":\"Figure\",\"type\":\"Plot\"}},\"id\":\"fc5cb3e4-da44-4816-8f03-f186c34959da\",\"type\":\"ResetTool\"},{\"attributes\":{},\"id\":\"6b30ed16-04bf-43b5-87db-37cf997e92c9\",\"type\":\"ToolEvents\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"b39cc0dd-b053-4d51-bdc6-39cc7bc3f5fe\",\"type\":\"PanTool\"},{\"id\":\"e1eae6f4-b285-40c3-a97b-e94215d6b263\",\"type\":\"WheelZoomTool\"},{\"id\":\"fc5cb3e4-da44-4816-8f03-f186c34959da\",\"type\":\"ResetTool\"},{\"id\":\"78ea573a-1253-437a-b777-16ad7c5fc3d3\",\"type\":\"SaveTool\"}]},\"id\":\"dd7151fb-d8d7-484c-a1bf-cfca93c3a34f\",\"type\":\"Toolbar\"},{\"attributes\":{\"plot\":{\"id\":\"fd87fb36-4a56-477c-b8b7-107ab7e5f56b\",\"subtype\":\"Figure\",\"type\":\"Plot\"}},\"id\":\"78ea573a-1253-437a-b777-16ad7c5fc3d3\",\"type\":\"SaveTool\"},{\"attributes\":{\"formatter\":{\"id\":\"9fc50889-5464-4d39-995d-eb976161d778\",\"type\":\"BasicTickFormatter\"},\"plot\":{\"id\":\"fd87fb36-4a56-477c-b8b7-107ab7e5f56b\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"829f4a6d-049b-4010-94b7-d587a0c51dd0\",\"type\":\"BasicTicker\"}},\"id\":\"1a6cab84-4756-4c66-b429-b3c59e15b790\",\"type\":\"LinearAxis\"},{\"attributes\":{\"plot\":{\"id\":\"fd87fb36-4a56-477c-b8b7-107ab7e5f56b\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"829f4a6d-049b-4010-94b7-d587a0c51dd0\",\"type\":\"BasicTicker\"}},\"id\":\"30a42cc3-61c2-42f3-bf39-6df2c79959e2\",\"type\":\"Grid\"},{\"attributes\":{\"dimension\":1,\"plot\":{\"id\":\"fd87fb36-4a56-477c-b8b7-107ab7e5f56b\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"296ea7e0-d9b9-4b34-9c2d-536801e12ecd\",\"type\":\"BasicTicker\"}},\"id\":\"65c152bb-6a92-4055-b104-de140d811ed5\",\"type\":\"Grid\"},{\"attributes\":{\"fill_color\":{\"field\":\"fill_color\"},\"line_color\":{\"field\":\"line_color\"},\"size\":{\"units\":\"screen\",\"value\":8},\"x\":{\"field\":\"x1\"},\"y\":{\"field\":\"x2\"}},\"id\":\"5caf80c8-27ae-4555-80ae-fcf86ae22fbf\",\"type\":\"Circle\"},{\"attributes\":{\"plot\":{\"id\":\"fd87fb36-4a56-477c-b8b7-107ab7e5f56b\",\"subtype\":\"Figure\",\"type\":\"Plot\"}},\"id\":\"b39cc0dd-b053-4d51-bdc6-39cc7bc3f5fe\",\"type\":\"PanTool\"},{\"attributes\":{\"callback\":null},\"id\":\"10f4d84c-11f4-44de-a545-92679213fa89\",\"type\":\"DataRange1d\"},{\"attributes\":{\"plot\":{\"id\":\"fd87fb36-4a56-477c-b8b7-107ab7e5f56b\",\"subtype\":\"Figure\",\"type\":\"Plot\"}},\"id\":\"e1eae6f4-b285-40c3-a97b-e94215d6b263\",\"type\":\"WheelZoomTool\"},{\"attributes\":{\"data_source\":{\"id\":\"3ddbb301-7cb2-4edd-aed7-b77feb56f67a\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"5caf80c8-27ae-4555-80ae-fcf86ae22fbf\",\"type\":\"Circle\"},\"hover_glyph\":null,\"nonselection_glyph\":{\"id\":\"c88cd9e4-6e22-4140-9ad5-d34178b38b55\",\"type\":\"Circle\"},\"selection_glyph\":null},\"id\":\"05523265-9e10-478a-be5c-fced408d95c6\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"size\":{\"units\":\"screen\",\"value\":8},\"x\":{\"field\":\"x1\"},\"y\":{\"field\":\"x2\"}},\"id\":\"c88cd9e4-6e22-4140-9ad5-d34178b38b55\",\"type\":\"Circle\"},{\"attributes\":{\"plot\":{\"id\":\"fd87fb36-4a56-477c-b8b7-107ab7e5f56b\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"source\":{\"id\":\"3ddbb301-7cb2-4edd-aed7-b77feb56f67a\",\"type\":\"ColumnDataSource\"},\"text\":{\"field\":\"names\"},\"text_align\":\"center\",\"text_color\":{\"value\":\"#555555\"},\"text_font_size\":{\"value\":\"8pt\"},\"x\":{\"field\":\"x1\"},\"y\":{\"field\":\"x2\"},\"y_offset\":{\"value\":6}},\"id\":\"b6aec15f-6b34-4f8e-970b-e221ba5855da\",\"type\":\"LabelSet\"}],\"root_ids\":[\"fd87fb36-4a56-477c-b8b7-107ab7e5f56b\"]},\"title\":\"Bokeh Application\",\"version\":\"0.12.4\"}};\n",
       "            var render_items = [{\"docid\":\"a71d9186-081f-4735-b10e-a42440bed059\",\"elementid\":\"431dd0c9-b460-488f-b329-bf72552c6a32\",\"modelid\":\"fd87fb36-4a56-477c-b8b7-107ab7e5f56b\"}];\n",
       "            \n",
       "            Bokeh.embed.embed_items(docs_json, render_items);\n",
       "          };\n",
       "          if (document.readyState != \"loading\") fn();\n",
       "          else document.addEventListener(\"DOMContentLoaded\", fn);\n",
       "        })();\n",
       "      },\n",
       "      function(Bokeh) {\n",
       "      }\n",
       "    ];\n",
       "  \n",
       "    function run_inline_js() {\n",
       "      \n",
       "      if ((window.Bokeh !== undefined) || (force === true)) {\n",
       "        for (var i = 0; i < inline_js.length; i++) {\n",
       "          inline_js[i](window.Bokeh);\n",
       "        }if (force === true) {\n",
       "          display_loaded();\n",
       "        }} else if (Date.now() < window._bokeh_timeout) {\n",
       "        setTimeout(run_inline_js, 100);\n",
       "      } else if (!window._bokeh_failed_load) {\n",
       "        console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "        window._bokeh_failed_load = true;\n",
       "      } else if (force !== true) {\n",
       "        var cell = $(document.getElementById(\"431dd0c9-b460-488f-b329-bf72552c6a32\")).parents('.cell').data().cell;\n",
       "        cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "      }\n",
       "  \n",
       "    }\n",
       "  \n",
       "    if (window._bokeh_is_loading === 0) {\n",
       "      console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "      run_inline_js();\n",
       "    } else {\n",
       "      load_libs(js_urls, function() {\n",
       "        console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "        run_inline_js();\n",
       "      });\n",
       "    }\n",
       "  }(this));\n",
       "</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p = figure(tools=\"pan,wheel_zoom,reset,save\",\n",
    "           toolbar_location=\"above\",\n",
    "           title=\"vector T-SNE for most polarized words\")\n",
    "\n",
    "source = ColumnDataSource(data=dict(x1=words_top_ted_tsne[:,0],\n",
    "                                    x2=words_top_ted_tsne[:,1],\n",
    "                                    names=words_to_visualize))\n",
    "\n",
    "p.scatter(x=\"x1\", y=\"x2\", size=8, source=source,color=colors_list)\n",
    "\n",
    "word_labels = LabelSet(x=\"x1\", y=\"x2\", text=\"names\", y_offset=6,\n",
    "                  text_font_size=\"8pt\", text_color=\"#555555\",\n",
    "                  source=source, text_align='center')\n",
    "p.add_layout(word_labels)\n",
    "\n",
    "show(p)\n",
    "\n",
    "# green indicates positive words, black indicates negative words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "After curating the dataset and training we were able to achieve a speed of a couple of hundred words per second with a very low accuracy. However after doing the first instance of noise reduction we were able to get the accuracy upwards of 80%. After optimising inefficiencys the network managed to achieve the same accuracy and a couple of thousand words per second. Finally a second set of noise reduction removed the useless data from the set such as names and punctuation. This acieved an even higher accuracy. After this step the network now has the ability to trade of accuracy for speed by cutting more words out of the vocabulary. This can be helpful for training over a much larger dataset. It also marginally increased speed by removing some of the data.\n",
    "\n",
    "After visualising the data it can clearly be seen that the network has successfully grouped the input words by sentiment. With a few agnostic variables normally consisting of names that slipped through. This could be imporoved by increading the cutoff."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
